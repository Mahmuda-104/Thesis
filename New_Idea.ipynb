{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport scipy.io\nimport numpy as np\nfrom scipy import signal\n\ndata_path = \"/kaggle/input/control1\"\nlowcut = 0.4 \nhighcut = 100 \nfs_original = 500\nfs_new = 250\n\ncontrol_data = []\ncontrol_path = os.path.join(data_path, 'Control')\n\nfor foldername in os.listdir(control_path):\n    subfolder_path = os.path.join(control_path, foldername)\n    control_data.append(subfolder_path)\n\nControl30 = []\n\nfor control_data_path in control_data:\n    n_epochs = 70 \n    start_epoch = 1\n    end_epoch = 110\n\n    control_arr = []\n\n    for i in range(start_epoch, end_epoch+1):\n        epoch_path = os.path.join(control_data_path, f\"trial{i}.mat\")\n        mat_data = scipy.io.loadmat(epoch_path)\n        mat_data1 = mat_data[\"trialData_i\"]    \n        #print(mat_data1.shape)\n        # Downsampling\n        num_samples_original = mat_data1.shape[-1]\n        num_samples_new = int(num_samples_original * fs_new / fs_original)\n        downsampled_data = signal.resample(mat_data1, num_samples_new, axis=-1)\n        #print(downsampled_data.shape)\n        \n        control_arr.append(downsampled_data)\n        Control30.append(downsampled_data)\n\nControl30 = np.array(Control30)\n\nprint(Control30.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T19:25:17.699344Z","iopub.execute_input":"2023-06-22T19:25:17.700005Z","iopub.status.idle":"2023-06-22T19:25:55.923588Z","shell.execute_reply.started":"2023-06-22T19:25:17.699971Z","shell.execute_reply":"2023-06-22T19:25:55.922290Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"(3300, 60, 1000)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport scipy.io\nimport numpy as np\nfrom scipy import signal\ndata_path = \"/kaggle/input/concussed1\"\nlowcut = 0.4 \nhighcut = 100 \nfs_original = 500\nfs_new = 250  \n\nconcussed_data = []\nconcussed_path = os.path.join(data_path, 'Concussed')\nfor foldername in os.listdir(concussed_path):\n    subfolder_path = os.path.join(concussed_path, foldername)\n    concussed_data.append(subfolder_path)\n\nConcussed52 = []\nfor concussed_data_path in concussed_data:\n    n_epochs = 70\n    start_epoch = 1\n    end_epoch = 110\n    concussed_arr = []\n\n    for i in range(start_epoch, end_epoch+1):\n        epoch_path = f\"{concussed_data_path}/trial{i}.mat\"\n        mat_data = scipy.io.loadmat(epoch_path)\n        mat_data1 = mat_data[\"trialData_i\"]\n        num_samples_original = mat_data1.shape[-1]\n        num_samples_new = int(num_samples_original * fs_new / fs_original)\n        downsampled_data = signal.resample(mat_data1, num_samples_new, axis=-1)\n        \n        concussed_arr.append(np.array(downsampled_data))\n        \n        Concussed52.append(downsampled_data)\n\nConcussed52 = np.array(Concussed52)\n\nprint( Concussed52.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T19:27:27.349468Z","iopub.execute_input":"2023-06-22T19:27:27.349886Z","iopub.status.idle":"2023-06-22T19:28:33.277146Z","shell.execute_reply.started":"2023-06-22T19:27:27.349855Z","shell.execute_reply":"2023-06-22T19:28:33.275835Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"(5390, 60, 1000)\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = np.concatenate((Control30, Concussed52), axis=0)\nlabels = np.concatenate((np.zeros(Control30.shape[0]), np.ones(Concussed52.shape[0])))","metadata":{"execution":{"iopub.status.busy":"2023-06-22T19:28:43.408612Z","iopub.execute_input":"2023-06-22T19:28:43.410130Z","iopub.status.idle":"2023-06-22T19:28:45.988097Z","shell.execute_reply.started":"2023-06-22T19:28:43.410070Z","shell.execute_reply":"2023-06-22T19:28:45.986427Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, random_state=42)\nmean = np.mean(X_train)\nstd = np.std(X_train)\n\nX_train = (X_train - mean) / std\nX_test = (X_test - mean) / std","metadata":{"execution":{"iopub.status.busy":"2023-06-22T19:29:13.386765Z","iopub.execute_input":"2023-06-22T19:29:13.387195Z","iopub.status.idle":"2023-06-22T19:29:18.688583Z","shell.execute_reply.started":"2023-06-22T19:29:13.387162Z","shell.execute_reply":"2023-06-22T19:29:18.687453Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n\nmodel = Sequential()\nmodel.add(Conv1D(32, 3, activation='relu', input_shape=(60, 1000)))\nmodel.add(MaxPooling1D(2))\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T19:29:50.121681Z","iopub.execute_input":"2023-06-22T19:29:50.123252Z","iopub.status.idle":"2023-06-22T19:29:50.242149Z","shell.execute_reply.started":"2023-06-22T19:29:50.123198Z","shell.execute_reply":"2023-06-22T19:29:50.241118Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train, epochs=10, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T19:31:03.670179Z","iopub.execute_input":"2023-06-22T19:31:03.671475Z","iopub.status.idle":"2023-06-22T19:32:28.707998Z","shell.execute_reply.started":"2023-06-22T19:31:03.671432Z","shell.execute_reply":"2023-06-22T19:32:28.706008Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Epoch 1/10\n218/218 [==============================] - 5s 24ms/step - loss: 0.1132 - accuracy: 0.9600\nEpoch 2/10\n218/218 [==============================] - 5s 22ms/step - loss: 0.0540 - accuracy: 0.9855\nEpoch 3/10\n218/218 [==============================] - 5s 25ms/step - loss: 0.0522 - accuracy: 0.9856\nEpoch 4/10\n218/218 [==============================] - 5s 24ms/step - loss: 0.0416 - accuracy: 0.9882\nEpoch 5/10\n218/218 [==============================] - 5s 23ms/step - loss: 0.0364 - accuracy: 0.9919\nEpoch 6/10\n218/218 [==============================] - 5s 24ms/step - loss: 0.3036 - accuracy: 0.9085\nEpoch 7/10\n218/218 [==============================] - 5s 23ms/step - loss: 0.1008 - accuracy: 0.9632\nEpoch 8/10\n218/218 [==============================] - 5s 23ms/step - loss: 0.1098 - accuracy: 0.9675\nEpoch 9/10\n218/218 [==============================] - 5s 24ms/step - loss: 0.0612 - accuracy: 0.9787\nEpoch 10/10\n218/218 [==============================] - 5s 24ms/step - loss: 0.0303 - accuracy: 0.9925\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7dc9846f1090>"},"metadata":{}}]},{"cell_type":"code","source":"_, accuracy = model.evaluate(X_test, y_test)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100))","metadata":{"execution":{"iopub.status.busy":"2023-06-22T19:32:28.710711Z","iopub.execute_input":"2023-06-22T19:32:28.711145Z","iopub.status.idle":"2023-06-22T19:32:30.370761Z","shell.execute_reply.started":"2023-06-22T19:32:28.711100Z","shell.execute_reply":"2023-06-22T19:32:30.369232Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"55/55 [==============================] - 1s 9ms/step - loss: 0.6686 - accuracy: 0.8533\nAccuracy: 85.33%\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_pred = np.round(y_pred).flatten()\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(cm)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T19:32:34.657899Z","iopub.execute_input":"2023-06-22T19:32:34.658701Z","iopub.status.idle":"2023-06-22T19:32:36.145093Z","shell.execute_reply.started":"2023-06-22T19:32:34.658620Z","shell.execute_reply":"2023-06-22T19:32:36.143721Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"55/55 [==============================] - 1s 10ms/step\nConfusion Matrix:\n[[508 176]\n [ 79 975]]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nreport = classification_report(y_test, y_pred)\nprint(\"Classification Report:\")\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T19:32:40.210677Z","iopub.execute_input":"2023-06-22T19:32:40.211109Z","iopub.status.idle":"2023-06-22T19:32:40.232636Z","shell.execute_reply.started":"2023-06-22T19:32:40.211078Z","shell.execute_reply":"2023-06-22T19:32:40.231436Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n         0.0       0.87      0.74      0.80       684\n         1.0       0.85      0.93      0.88      1054\n\n    accuracy                           0.85      1738\n   macro avg       0.86      0.83      0.84      1738\nweighted avg       0.85      0.85      0.85      1738\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Control30.shape)\nprint(Control19.shape)\nControl30 = np.concatenate((Control30, Control19), axis=0)\nprint(Control30.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T16:55:10.080789Z","iopub.execute_input":"2023-06-22T16:55:10.081171Z","iopub.status.idle":"2023-06-22T16:55:10.349753Z","shell.execute_reply.started":"2023-06-22T16:55:10.081134Z","shell.execute_reply":"2023-06-22T16:55:10.348391Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(30, 36, 60, 1000)\n(19, 36, 60, 1000)\n(49, 36, 60, 1000)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport json\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import entropy\n\ndelta_band = (0.5, 4)\nalpha_band = (8, 12)\nbeta_band = (12, 35)\n\nall_control_power = []\nscaler = MinMaxScaler()\nepsilon = 1e-10\n\n# Normalizing the features\nfor i in range(len(Control30)):\n    control_data = Control30[i]\n    control_power = []\n    for epoch in control_data:\n        epoch_power = []\n        for channel_data in epoch:\n            freq_spectrum = np.fft.fft(channel_data)\n            power_spectrum = np.abs(freq_spectrum) ** 2\n\n            alpha_power = np.sum(power_spectrum[(alpha_band[0] <= freq_spectrum) & (freq_spectrum <= alpha_band[1])])\n            beta_power = np.sum(power_spectrum[(beta_band[0] <= freq_spectrum) & (freq_spectrum <= beta_band[1])])\n            delta_power = np.sum(power_spectrum[(delta_band[0] <= freq_spectrum) & (freq_spectrum <= delta_band[1])])\n\n            channel_min_value = np.min(channel_data)\n            channel_max_value = np.max(channel_data)\n            power_spectrum_adjusted = power_spectrum + epsilon\n            entropy_values = entropy(power_spectrum_adjusted)\n            values=np.array([alpha_power,beta_power,delta_power,entropy_values])\n            rescaled_values = (values - values.min()) * (channel_max_value - channel_min_value) / (values.max() - values.min()) + channel_min_value\n            concatenated_data = np.concatenate((channel_data, rescaled_values))\n            epoch_power.append(concatenated_data.tolist())\n\n        control_power.append(np.array(epoch_power))\n\n    all_control_power.append(np.array(control_power))\n\nall_control_power = np.array(all_control_power)\nprint(all_control_power.shape)\nprint(len(Control30))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T16:55:10.353265Z","iopub.execute_input":"2023-06-22T16:55:10.353654Z","iopub.status.idle":"2023-06-22T16:55:57.835017Z","shell.execute_reply.started":"2023-06-22T16:55:10.353623Z","shell.execute_reply":"2023-06-22T16:55:57.833702Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"(49, 36, 60, 1004)\n49\n","output_type":"stream"}]},{"cell_type":"code","source":"# Control30_part1 = Control30[:, :, :, :500]\n# Control30_part2 = Control30[:, :, :, :500]\n# print(\"Shape of Control30_part1:\", Control30_part1.shape)\n# print(\"Shape of Control30_part2:\", Control30_part2.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T16:55:57.837143Z","iopub.execute_input":"2023-06-22T16:55:57.837682Z","iopub.status.idle":"2023-06-22T16:55:57.842693Z","shell.execute_reply.started":"2023-06-22T16:55:57.837638Z","shell.execute_reply":"2023-06-22T16:55:57.841535Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import json\n# from sklearn.preprocessing import MinMaxScaler\n# from scipy.stats import entropy\n\n# delta_band = (0.5, 4)\n# alpha_band = (8, 12)\n# beta_band = (12, 35)\n\n# all_control_power = []\n# scaler = MinMaxScaler()\n# epsilon = 1e-10\n\n# # Divide the EEG data into two parts\n# Control30_part1 = Control30[:, :, :, :500]\n# Control30_part2 = Control30[:, :, :, :500]\n\n# # Calculate features for Control30_part1\n# all_control_power_part1 = []\n# for i in range(len(Control30_part1)):\n#     control_data = Control30_part1[i]\n#     control_power = []\n#     for epoch in control_data:\n#         epoch_power = []\n#         for channel_data in epoch:\n#             freq_spectrum = np.fft.fft(channel_data)\n#             power_spectrum = np.abs(freq_spectrum) ** 2\n\n#             alpha_power = np.sum(power_spectrum[(alpha_band[0] <= freq_spectrum) & (freq_spectrum <= alpha_band[1])])\n#             beta_power = np.sum(power_spectrum[(beta_band[0] <= freq_spectrum) & (freq_spectrum <= beta_band[1])])\n#             delta_power = np.sum(power_spectrum[(delta_band[0] <= freq_spectrum) & (freq_spectrum <= delta_band[1])])\n\n#             channel_min_value = np.min(channel_data)\n#             channel_max_value = np.max(channel_data)\n\n#             power_spectrum_adjusted = power_spectrum + epsilon\n#             entropy_values = entropy(power_spectrum_adjusted)\n#             values = np.array([alpha_power, beta_power, delta_power, entropy_values])\n#             rescaled_values = (values - values.min()) * (channel_max_value - channel_min_value) / (values.max() - values.min()) + channel_min_value\n#             concatenated_data = np.concatenate((channel_data, rescaled_values))\n\n#             epoch_power.append(concatenated_data.tolist())\n\n#         control_power.append(np.array(epoch_power))\n\n#     all_control_power_part1.append(np.array(control_power))\n\n# all_control_power_part1 = np.array(all_control_power_part1)\n# print(\"Shape of all_control_power_part1:\", all_control_power_part1.shape)\n\n# # Calculate features for Control30_part2\n# all_control_power_part2 = []\n# for i in range(len(Control30_part2)):\n#     control_data = Control30_part2[i]\n#     control_power = []\n#     for epoch in control_data:\n#         epoch_power = []\n#         for channel_data in epoch:\n#             freq_spectrum = np.fft.fft(channel_data)\n#             power_spectrum = np.abs(freq_spectrum) ** 2\n\n#             alpha_power = np.sum(power_spectrum[(alpha_band[0] <= freq_spectrum) & (freq_spectrum <= alpha_band[1])])\n#             beta_power = np.sum(power_spectrum[(beta_band[0] <= freq_spectrum) & (freq_spectrum <= beta_band[1])])\n#             delta_power = np.sum(power_spectrum[(delta_band[0] <= freq_spectrum) & (freq_spectrum <= delta_band[1])])\n\n#             channel_min_value = np.min(channel_data)\n#             channel_max_value = np.max(channel_data)\n\n#             power_spectrum_adjusted = power_spectrum + epsilon\n#             entropy_values = entropy(power_spectrum_adjusted)\n#             values = np.array([alpha_power, beta_power, delta_power, entropy_values])\n#             rescaled_values = (values - values.min()) * (channel_max_value - channel_min_value) / (values.max() - values.min()) + channel_min_value\n#             concatenated_data = np.concatenate((channel_data, rescaled_values))\n\n#             epoch_power.append(concatenated_data.tolist())\n\n#         control_power.append(np.array(epoch_power))\n\n#     all_control_power_part2.append(np.array(control_power))\n\n# all_control_power_part2 = np.array(all_control_power_part2)\n# print(\"Shape of all_control_power_part2:\", all_control_power_part2.shape)\n\n# # Concatenate the features along the second axis\n# Combined_data_Control = np.concatenate((all_control_power_part1, all_control_power_part2), axis=1)\n# print(\"Shape of Combined_data:\", Combined_data_Control.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T16:55:57.844535Z","iopub.execute_input":"2023-06-22T16:55:57.845336Z","iopub.status.idle":"2023-06-22T16:55:57.857977Z","shell.execute_reply.started":"2023-06-22T16:55:57.845265Z","shell.execute_reply":"2023-06-22T16:55:57.856801Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import os\nimport scipy.io\nimport numpy as np\nfrom scipy import signal\ndata_path = \"/kaggle/input/concussed1\"\nlowcut = 0.4 \nhighcut = 100 \nfs_original = 500\nfs_new = 250  \n\nconcussed_data = []\nconcussed_path = os.path.join(data_path, 'Concussed')\nfor foldername in os.listdir(concussed_path):\n    subfolder_path = os.path.join(concussed_path, foldername)\n    concussed_data.append(subfolder_path)\n\nConcussed52 = []\nfor concussed_data_path in concussed_data:\n    n_epochs = 70\n    start_epoch = 35\n    end_epoch = 70\n    concussed_arr = []\n\n    for i in range(start_epoch, end_epoch+1):\n        epoch_path = f\"{concussed_data_path}/trial{i}.mat\"\n        mat_data = scipy.io.loadmat(epoch_path)\n        mat_data1 = mat_data[\"trialData_i\"]\n        num_samples_original = mat_data1.shape[-1]\n        num_samples_new = int(num_samples_original * fs_new / fs_original)\n        downsampled_data = signal.resample(mat_data1, num_samples_new, axis=-1)\n        \n        concussed_arr.append(np.array(downsampled_data))\n        \n        Concussed52.append(downsampled_data)\n\nConcussed52 = np.array(Concussed52)\n\nprint( Concussed52.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T19:17:08.773420Z","iopub.execute_input":"2023-06-22T19:17:08.773798Z","iopub.status.idle":"2023-06-22T19:17:19.186629Z","shell.execute_reply.started":"2023-06-22T19:17:08.773770Z","shell.execute_reply":"2023-06-22T19:17:19.185476Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(1764, 60, 1000)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-06-22T19:19:28.569112Z","iopub.execute_input":"2023-06-22T19:19:28.569538Z","iopub.status.idle":"2023-06-22T19:19:28.950603Z","shell.execute_reply.started":"2023-06-22T19:19:28.569508Z","shell.execute_reply":"2023-06-22T19:19:28.949458Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-06-22T19:20:49.158635Z","iopub.execute_input":"2023-06-22T19:20:49.159028Z","iopub.status.idle":"2023-06-22T19:20:59.444169Z","shell.execute_reply.started":"2023-06-22T19:20:49.159001Z","shell.execute_reply":"2023-06-22T19:20:59.443058Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"model.fit(X_train, y_train, epochs=10, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T19:21:09.923892Z","iopub.execute_input":"2023-06-22T19:21:09.924368Z","iopub.status.idle":"2023-06-22T19:21:32.266353Z","shell.execute_reply.started":"2023-06-22T19:21:09.924332Z","shell.execute_reply":"2023-06-22T19:21:32.264991Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch 1/10\n72/72 [==============================] - 3s 25ms/step - loss: 0.7033 - accuracy: 0.6202\nEpoch 2/10\n72/72 [==============================] - 2s 23ms/step - loss: 0.5275 - accuracy: 0.7196\nEpoch 3/10\n72/72 [==============================] - 2s 24ms/step - loss: 0.4849 - accuracy: 0.7802\nEpoch 4/10\n72/72 [==============================] - 2s 24ms/step - loss: 0.4031 - accuracy: 0.8207\nEpoch 5/10\n72/72 [==============================] - 2s 23ms/step - loss: 0.3634 - accuracy: 0.8426\nEpoch 6/10\n72/72 [==============================] - 2s 22ms/step - loss: 0.2608 - accuracy: 0.9055\nEpoch 7/10\n72/72 [==============================] - 2s 22ms/step - loss: 0.1858 - accuracy: 0.9354\nEpoch 8/10\n72/72 [==============================] - 2s 25ms/step - loss: 0.1270 - accuracy: 0.9631\nEpoch 9/10\n72/72 [==============================] - 2s 25ms/step - loss: 0.0959 - accuracy: 0.9749\nEpoch 10/10\n72/72 [==============================] - 2s 24ms/step - loss: 0.0636 - accuracy: 0.9877\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7dc9146d58a0>"},"metadata":{}}]},{"cell_type":"code","source":"_, accuracy = model.evaluate(X_test, y_test)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100))","metadata":{"execution":{"iopub.status.busy":"2023-06-22T19:21:50.906642Z","iopub.execute_input":"2023-06-22T19:21:50.907089Z","iopub.status.idle":"2023-06-22T19:21:51.387652Z","shell.execute_reply.started":"2023-06-22T19:21:50.907058Z","shell.execute_reply":"2023-06-22T19:21:51.386332Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"18/18 [==============================] - 0s 9ms/step - loss: 0.6896 - accuracy: 0.7944\nAccuracy: 79.44%\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_pred = np.round(y_pred).flatten()\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(cm)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T19:22:33.724368Z","iopub.execute_input":"2023-06-22T19:22:33.724853Z","iopub.status.idle":"2023-06-22T19:22:34.372819Z","shell.execute_reply.started":"2023-06-22T19:22:33.724819Z","shell.execute_reply":"2023-06-22T19:22:34.371355Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"18/18 [==============================] - 0s 9ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nreport = classification_report(y_test, y_pred)\nprint(\"Classification Report:\")\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T19:24:09.861031Z","iopub.execute_input":"2023-06-22T19:24:09.861627Z","iopub.status.idle":"2023-06-22T19:24:09.882688Z","shell.execute_reply.started":"2023-06-22T19:24:09.861585Z","shell.execute_reply":"2023-06-22T19:24:09.881607Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n         0.0       0.75      0.69      0.72       217\n         1.0       0.82      0.86      0.84       352\n\n    accuracy                           0.79       569\n   macro avg       0.78      0.77      0.78       569\nweighted avg       0.79      0.79      0.79       569\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Concussed52_part1 = Concussed52[:, :, :, :500]\n# Concussed52_part2 = Concussed52[:, :, :, :500]\n# print(\"Shape of Concussed52_part1:\", Concussed52_part1.shape)\n# print(\"Shape of Concussed52_part2:\", Concussed52_part2.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T16:56:23.565812Z","iopub.execute_input":"2023-06-22T16:56:23.566269Z","iopub.status.idle":"2023-06-22T16:56:23.572425Z","shell.execute_reply.started":"2023-06-22T16:56:23.566224Z","shell.execute_reply":"2023-06-22T16:56:23.571032Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import json\n# from sklearn.preprocessing import MinMaxScaler\n# from scipy.stats import entropy\n\n# delta_band = (0.5, 4)\n# alpha_band = (8, 12)\n# beta_band = (12, 35)\n\n# all_control_power = []\n# scaler = MinMaxScaler()\n# epsilon = 1e-10\n\n# # Divide the EEG data into two parts\n# Concussed52_part1 = Concussed52[:, :, :, :500]\n# Concussed52_part2 = Concussed52[:, :, :, :500]\n\n# # Calculate features for Concussed52_part1\n# all_concussed_power_part1 = []\n# for i in range(len(Concussed52_part1)):\n#     concussed_data = Concussed52_part1[i]\n#     concussed_power = []\n#     for epoch in concussed_data:\n#         epoch_power = []\n#         for channel_data in epoch:\n#             freq_spectrum = np.fft.fft(channel_data)\n#             power_spectrum = np.abs(freq_spectrum) ** 2\n\n#             alpha_power = np.sum(power_spectrum[(alpha_band[0] <= freq_spectrum) & (freq_spectrum <= alpha_band[1])])\n#             beta_power = np.sum(power_spectrum[(beta_band[0] <= freq_spectrum) & (freq_spectrum <= beta_band[1])])\n#             delta_power = np.sum(power_spectrum[(delta_band[0] <= freq_spectrum) & (freq_spectrum <= delta_band[1])])\n\n#             channel_min_value = np.min(channel_data)\n#             channel_max_value = np.max(channel_data)\n\n#             power_spectrum_adjusted = power_spectrum + epsilon\n#             entropy_values = entropy(power_spectrum_adjusted)\n#             values = np.array([alpha_power, beta_power, delta_power, entropy_values])\n#             rescaled_values = (values - values.min()) * (channel_max_value - channel_min_value) / (values.max() - values.min()) + channel_min_value\n#             concatenated_data = np.concatenate((channel_data, rescaled_values))\n\n#             epoch_power.append(concatenated_data.tolist())\n\n#         concussed_power.append(np.array(epoch_power))\n\n#     all_concussed_power_part1.append(np.array(concussed_power))\n\n# all_concussed_power_part1 = np.array(all_concussed_power_part1)\n# print(\"Shape of all_concussed_power_part1:\", all_concussed_power_part1.shape)\n\n# # Calculate features for Concussed52_part2\n# all_concussed_power_part2 = []\n# for i in range(len(Concussed52_part2)):\n#     concussed_data = Concussed52_part2[i]\n#     concussed_power = []\n#     for epoch in concussed_data:\n#         epoch_power = []\n#         for channel_data in epoch:\n#             freq_spectrum = np.fft.fft(channel_data)\n#             power_spectrum = np.abs(freq_spectrum) ** 2\n\n#             alpha_power = np.sum(power_spectrum[(alpha_band[0] <= freq_spectrum) & (freq_spectrum <= alpha_band[1])])\n#             beta_power = np.sum(power_spectrum[(beta_band[0] <= freq_spectrum) & (freq_spectrum <= beta_band[1])])\n#             delta_power = np.sum(power_spectrum[(delta_band[0] <= freq_spectrum) & (freq_spectrum <= delta_band[1])])\n\n#             channel_min_value = np.min(channel_data)\n#             channel_max_value = np.max(channel_data)\n\n#             power_spectrum_adjusted = power_spectrum + epsilon\n#             entropy_values = entropy(power_spectrum_adjusted)\n#             values = np.array([alpha_power, beta_power, delta_power, entropy_values])\n#             rescaled_values = (values - values.min()) * (channel_max_value - channel_min_value) / (values.max() - values.min()) + channel_min_value\n#             concatenated_data = np.concatenate((channel_data, rescaled_values))\n\n#             epoch_power.append(concatenated_data.tolist())\n\n#         concussed_power.append(np.array(epoch_power))\n\n#     all_concussed_power_part2.append(np.array(concussed_power))\n\n# all_concussed_power_part2 = np.array(all_concussed_power_part2)\n# print(\"Shape of all_concussed_power_part2:\", all_concussed_power_part2.shape)\n\n# # Concatenate the features from both parts along the second axis\n# Combined_data_Concussed = np.concatenate((all_concussed_power_part1, all_concussed_power_part2), axis=1)\n# print(\"Shape of Combined_data:\", Combined_data_Concussed.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T16:56:23.574304Z","iopub.execute_input":"2023-06-22T16:56:23.574789Z","iopub.status.idle":"2023-06-22T16:56:23.588281Z","shell.execute_reply.started":"2023-06-22T16:56:23.574745Z","shell.execute_reply":"2023-06-22T16:56:23.587126Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport json\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import entropy\n\ndelta_band = (0.5, 4)\nalpha_band = (8, 12)\nbeta_band = (12, 35)\n\nall_concussed_power = []\nscaler = MinMaxScaler()\nepsilon = 1e-10\n\nfor i in range(len(Concussed52)):\n    concussed_data = Concussed52[i]\n    concussed_power = []\n    for epoch in concussed_data:\n        epoch_power = []\n        for channel_data in epoch:\n            freq_spectrum = np.fft.fft(channel_data)\n            power_spectrum = np.abs(freq_spectrum) ** 2\n\n            alpha_power = np.sum(power_spectrum[(alpha_band[0] <= freq_spectrum) & (freq_spectrum <= alpha_band[1])])\n            beta_power = np.sum(power_spectrum[(beta_band[0] <= freq_spectrum) & (freq_spectrum <= beta_band[1])])\n            delta_power = np.sum(power_spectrum[(delta_band[0] <= freq_spectrum) & (freq_spectrum <= delta_band[1])])\n\n            channel_min_value = np.min(channel_data)\n            channel_max_value = np.max(channel_data)\n            power_spectrum_adjusted = power_spectrum + epsilon\n            entropy_values = entropy(power_spectrum_adjusted)\n            values=np.array([alpha_power,beta_power,delta_power,entropy_values])\n            rescaled_values = (values - values.min()) * (channel_max_value - channel_min_value) / (values.max() - values.min()) + channel_min_value\n            concatenated_data = np.concatenate((channel_data, rescaled_values))\n            epoch_power.append(concatenated_data.tolist())\n\n        concussed_power.append(np.array(epoch_power))\n\n    all_concussed_power.append(np.array(concussed_power))\n\nall_concussed_power = np.array(all_concussed_power)\nprint(all_concussed_power.shape)\nprint(len(Concussed52))","metadata":{"execution":{"iopub.status.busy":"2023-06-22T16:56:23.589529Z","iopub.execute_input":"2023-06-22T16:56:23.589857Z","iopub.status.idle":"2023-06-22T16:57:11.047619Z","shell.execute_reply.started":"2023-06-22T16:56:23.589830Z","shell.execute_reply":"2023-06-22T16:57:11.046232Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"(49, 36, 60, 1004)\n49\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_control_train, x_control_test, y_control_train, y_control_test = train_test_split(all_control_power, np.zeros((49,)), test_size=0.2, random_state=42)\nx_control_train, x_control_val, y_control_train, y_control_val = train_test_split(x_control_train, y_control_train, test_size=0.2, random_state=42)\n\nx_concussed_train, x_concussed_test, y_concussed_train, y_concussed_test = train_test_split(all_concussed_power, np.ones((49,)), test_size=0.2, random_state=42)\nx_concussed_train, x_concussed_val, y_concussed_train, y_concussed_val = train_test_split(x_concussed_train, y_concussed_train, test_size=0.2, random_state=42)\n\n# Combine the control and concussed data\nx_train = np.concatenate((x_control_train, x_concussed_train), axis=0)\ny_train = np.concatenate((y_control_train, y_concussed_train), axis=0)\nx_val = np.concatenate((x_control_val, x_concussed_val), axis=0)\ny_val = np.concatenate((y_control_val, y_concussed_val), axis=0)\nx_test = np.concatenate((x_control_test, x_concussed_test), axis=0)\ny_test = np.concatenate((y_control_test, y_concussed_test), axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T16:57:11.049219Z","iopub.execute_input":"2023-06-22T16:57:11.049606Z","iopub.status.idle":"2023-06-22T16:57:13.558862Z","shell.execute_reply.started":"2023-06-22T16:57:11.049575Z","shell.execute_reply":"2023-06-22T16:57:13.557852Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nx_train_normalized = scaler.fit_transform(x_train.reshape(-1, 1004)).reshape(x_train.shape)\nx_val_normalized = scaler.transform(x_val.reshape(-1, 1004)).reshape(x_val.shape)\nx_test_normalized = scaler.transform(x_test.reshape(-1, 1004)).reshape(x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T16:57:13.560682Z","iopub.execute_input":"2023-06-22T16:57:13.561850Z","iopub.status.idle":"2023-06-22T16:57:16.843957Z","shell.execute_reply.started":"2023-06-22T16:57:13.561808Z","shell.execute_reply":"2023-06-22T16:57:16.842698Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from keras.utils import to_categorical\n\ny_train_categorical = to_categorical(y_train)\ny_val_categorical = to_categorical(y_val)\ny_test_categorical = to_categorical(y_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T16:57:16.850959Z","iopub.execute_input":"2023-06-22T16:57:16.851381Z","iopub.status.idle":"2023-06-22T16:57:20.498965Z","shell.execute_reply.started":"2023-06-22T16:57:16.851345Z","shell.execute_reply":"2023-06-22T16:57:20.497458Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:14:51.982631Z","iopub.execute_input":"2023-06-22T17:14:51.983058Z","iopub.status.idle":"2023-06-22T17:16:05.596822Z","shell.execute_reply.started":"2023-06-22T17:14:51.983024Z","shell.execute_reply":"2023-06-22T17:16:05.595421Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Epoch 1/50\n1/1 [==============================] - 6s 6s/step - loss: 0.6977 - accuracy: 0.4839 - val_loss: 0.7280 - val_accuracy: 0.5000\nEpoch 2/50\n1/1 [==============================] - 4s 4s/step - loss: 0.6866 - accuracy: 0.6129 - val_loss: 0.7247 - val_accuracy: 0.5000\nEpoch 3/50\n1/1 [==============================] - 4s 4s/step - loss: 0.8293 - accuracy: 0.5968 - val_loss: 0.7216 - val_accuracy: 0.5000\nEpoch 4/50\n1/1 [==============================] - 4s 4s/step - loss: 1.3549 - accuracy: 0.5000 - val_loss: 0.7206 - val_accuracy: 0.5000\nEpoch 5/50\n1/1 [==============================] - 4s 4s/step - loss: 0.6764 - accuracy: 0.4516 - val_loss: 0.7196 - val_accuracy: 0.5000\nEpoch 6/50\n1/1 [==============================] - 4s 4s/step - loss: 0.7185 - accuracy: 0.5968 - val_loss: 0.7182 - val_accuracy: 0.5000\nEpoch 7/50\n1/1 [==============================] - 4s 4s/step - loss: 0.6707 - accuracy: 0.5645 - val_loss: 0.7167 - val_accuracy: 0.5000\nEpoch 8/50\n1/1 [==============================] - 8s 8s/step - loss: 0.7265 - accuracy: 0.4355 - val_loss: 0.7159 - val_accuracy: 0.5000\nEpoch 9/50\n1/1 [==============================] - 7s 7s/step - loss: 0.6975 - accuracy: 0.5645 - val_loss: 0.7156 - val_accuracy: 0.5625\nEpoch 10/50\n1/1 [==============================] - 6s 6s/step - loss: 1.1693 - accuracy: 0.5323 - val_loss: 0.7156 - val_accuracy: 0.5625\nEpoch 11/50\n1/1 [==============================] - 7s 7s/step - loss: 0.6635 - accuracy: 0.5323 - val_loss: 0.7157 - val_accuracy: 0.5625\nEpoch 12/50\n1/1 [==============================] - 4s 4s/step - loss: 0.7760 - accuracy: 0.4516 - val_loss: 0.7159 - val_accuracy: 0.5625\nEpoch 13/50\n1/1 [==============================] - 4s 4s/step - loss: 1.5406 - accuracy: 0.5806 - val_loss: 0.7165 - val_accuracy: 0.5625\nEpoch 14/50\n1/1 [==============================] - 4s 4s/step - loss: 0.6673 - accuracy: 0.5000 - val_loss: 0.7170 - val_accuracy: 0.5625\nEpoch 15/50\n1/1 [==============================] - 4s 4s/step - loss: 0.6538 - accuracy: 0.6452 - val_loss: 0.7173 - val_accuracy: 0.5000\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x79ebf4767580>"},"metadata":{}}]},{"cell_type":"code","source":"loss, accuracy = model.evaluate(x_test_normalized, y_test_categorical)\nprint('Test loss:', loss)\nprint('Test accuracy:', accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:16:16.227613Z","iopub.execute_input":"2023-06-22T17:16:16.228648Z","iopub.status.idle":"2023-06-22T17:16:17.039736Z","shell.execute_reply.started":"2023-06-22T17:16:16.228550Z","shell.execute_reply":"2023-06-22T17:16:17.038403Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 416ms/step - loss: 0.6499 - accuracy: 0.5000\nTest loss: 0.64994215965271\nTest accuracy: 0.5\n","output_type":"stream"}]},{"cell_type":"code","source":"fggj","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:45.591292Z","iopub.execute_input":"2023-06-22T17:01:45.592427Z","iopub.status.idle":"2023-06-22T17:01:46.523703Z","shell.execute_reply.started":"2023-06-22T17:01:45.592390Z","shell.execute_reply":"2023-06-22T17:01:46.521067Z"},"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfggj\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name 'fggj' is not defined"],"ename":"NameError","evalue":"name 'fggj' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\ndata_combined = np.concatenate((all_control_power, all_concussed_power), axis=0)\n\ncontrol_labels = np.zeros(all_control_power.shape[0])\nconcussed_labels = np.ones(all_concussed_power.shape[0])\nlabels_combined = np.concatenate((control_labels, concussed_labels))\n\nx_train_val, x_test, y_train_val, y_test = train_test_split(data_combined, labels_combined, test_size=0.2, stratify=labels_combined, random_state=42)\nx_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2, stratify=y_train_val, random_state=42)\nscaler = MinMaxScaler()\n\nx_train_flat = x_train.reshape(x_train.shape[0], -1)\nx_val_flat = x_val.reshape(x_val.shape[0], -1)\nx_test_flat = x_test.reshape(x_test.shape[0], -1)\nx_train_normalized = scaler.fit_transform(x_train_flat)\nx_val_normalized = scaler.transform(x_val_flat)\nx_test_normalized = scaler.transform(x_test_flat)\n\nx_train_normalized = x_train_normalized.reshape(x_train.shape)\nx_val_normalized = x_val_normalized.reshape(x_val.shape)\nx_test_normalized = x_test_normalized.reshape(x_test.shape)\nprint(\"x_train shape:\", x_train_normalized.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"x_val shape:\", x_val_normalized.shape)\nprint(\"y_val shape:\", y_val.shape)\nprint(\"x_test shape:\", x_test_normalized.shape)\nprint(\"y_test shape:\", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.526299Z","iopub.status.idle":"2023-06-22T17:01:46.526991Z","shell.execute_reply.started":"2023-06-22T17:01:46.526622Z","shell.execute_reply":"2023-06-22T17:01:46.526642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, GRU, Dense, Dropout, Bidirectional\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom keras import regularizers\n\nx_train_reshaped = x_train_normalized.reshape(x_train_normalized.shape[0], -1)\nx_test_reshaped = x_test_normalized.reshape(x_test_normalized.shape[0], -1)\nscaler = StandardScaler()\nx_train_scaled = scaler.fit_transform(x_train_reshaped)\nx_test_scaled = scaler.transform(x_test_reshaped)\n\ny_train_categorical = to_categorical(y_train)\ny_test_categorical = to_categorical(y_test)\nx_train_scaled = x_train_scaled.reshape(x_train_normalized.shape[0], x_train_normalized.shape[1], -1)\nx_test_scaled = x_test_scaled.reshape(x_test_normalized.shape[0], x_test_normalized.shape[1], -1)\n\nmodel = Sequential()\nmodel.add(Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=regularizers.l2(0.01)),\n                        input_shape=(x_train_normalized.shape[1], x_train_scaled.shape[2])))\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(LSTM(units=256, return_sequences=True, kernel_regularizer=regularizers.l2(0.01))))\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(LSTM(units=128, return_sequences=True, kernel_regularizer=regularizers.l2(0.01))))\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(GRU(units=64, return_sequences=True, kernel_regularizer=regularizers.l2(0.01))))\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(GRU(units=32, kernel_regularizer=regularizers.l2(0.01))))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=16, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dense(units=2, activation='softmax'))\n\nlearning_rate = 0.0006\noptimizer = Adam(learning_rate=learning_rate)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\nearly_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\nhistory = model.fit(x_train_scaled, y_train_categorical, validation_split=0.2,\n                    epochs=200, batch_size=64, callbacks=[early_stopping], verbose=0)\n\nloss, accuracy = model.evaluate(x_test_scaled, y_test_categorical)\nprint('Test loss:', loss)\nprint('Test accuracy:', accuracy)\n\ntest_predictions = model.predict(x_test_scaled)\ntest_predicted_labels = np.argmax(test_predictions, axis=1)\ntest_true_labels = np.argmax(y_test_categorical, axis=1)\n\ncm_test = confusion_matrix(test_true_labels, test_predicted_labels)\nprint('Confusion Matrix (Test):')\nprint(cm_test)\n\nclass_report_test = classification_report(test_true_labels, test_predicted_labels)\nprint('Classification Report (Test):')\nprint(class_report_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.530430Z","iopub.status.idle":"2023-06-22T17:01:46.531178Z","shell.execute_reply.started":"2023-06-22T17:01:46.530772Z","shell.execute_reply":"2023-06-22T17:01:46.530799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = np.concatenate((Combined_data_Control, Combined_data_Concussed), axis=0)\nlabels = np.concatenate((np.zeros(len(Combined_data_Control)), np.ones(len(Combined_data_Concussed))))","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.533952Z","iopub.status.idle":"2023-06-22T17:01:46.534660Z","shell.execute_reply.started":"2023-06-22T17:01:46.534277Z","shell.execute_reply":"2023-06-22T17:01:46.534352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(dataset, labels, test_size=0.2, random_state=42)\nprint(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_val shape:\", X_val.shape)\nprint(\"y_val shape:\", y_val.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.537713Z","iopub.status.idle":"2023-06-22T17:01:46.538385Z","shell.execute_reply.started":"2023-06-22T17:01:46.538034Z","shell.execute_reply":"2023-06-22T17:01:46.538069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\n# Define the model architecture\nmodel = Sequential()\n\n# Convolutional layers\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(140,60,504)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Flatten layer\nmodel.add(Flatten())\n\n# Fully connected layers\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Print the model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.540949Z","iopub.status.idle":"2023-06-22T17:01:46.541783Z","shell.execute_reply.started":"2023-06-22T17:01:46.541412Z","shell.execute_reply":"2023-06-22T17:01:46.541451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\ncallback_list = EarlyStopping(\n        monitor='val_loss',\n        patience=20,\n        restore_best_weights=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.543662Z","iopub.status.idle":"2023-06-22T17:01:46.544194Z","shell.execute_reply.started":"2023-06-22T17:01:46.543912Z","shell.execute_reply":"2023-06-22T17:01:46.543937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    loss='binary_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n    metrics=['accuracy']\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.546749Z","iopub.status.idle":"2023-06-22T17:01:46.547373Z","shell.execute_reply.started":"2023-06-22T17:01:46.547053Z","shell.execute_reply":"2023-06-22T17:01:46.547088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nX =dataset\ny = labels \nEpoch = 10\nbatchSize = 256\n\n# Define the number of folds for cross-validation\nk_folds = 3\nkf = KFold(n_splits=k_folds, shuffle=True)\n\ntrain_accuracies = []\nval_accuracies = []\n\ntrue_labels = []\npredicted_labels = []\n\n\n# cross-validation\nfor train_index, val_index in kf.split(X):\n    X_train, X_val = X[train_index], X[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    history = model.fit(\n    X_train,\n    y_train,\n    epochs=Epoch,\n    batch_size=batchSize,\n    validation_data=(X_val, y_val),\n    callbacks=callback_list,\n    verbose=1)\n\n    # Evaluate the model on training and validation data\n    train_loss, train_accuracy = model.evaluate(X_train, y_train)\n    val_loss, val_accuracy = model.evaluate(X_val, y_val)\n\n    train_accuracies.append(train_accuracy)\n    val_accuracies.append(val_accuracy)\n    \n    val_predictions = model.predict(X_val)\n    val_predictions = np.argmax(val_predictions, axis=1)\n    true_labels.extend(y_val)\n    predicted_labels.extend(val_predictions)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.551415Z","iopub.status.idle":"2023-06-22T17:01:46.552204Z","shell.execute_reply.started":"2023-06-22T17:01:46.551775Z","shell.execute_reply":"2023-06-22T17:01:46.551837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\n\n# Convert the true labels and predicted labels to numpy arrays\ntrue_labels = np.array(true_labels)\npredicted_labels = np.array(predicted_labels)\n\n# Calculate the confusion matrix\ncm = confusion_matrix(true_labels, predicted_labels)\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# Calculate the classification report\nreport = classification_report(true_labels, predicted_labels)\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.554456Z","iopub.status.idle":"2023-06-22T17:01:46.555027Z","shell.execute_reply.started":"2023-06-22T17:01:46.554750Z","shell.execute_reply":"2023-06-22T17:01:46.554805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"New One","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.557004Z","iopub.status.idle":"2023-06-22T17:01:46.557825Z","shell.execute_reply.started":"2023-06-22T17:01:46.557423Z","shell.execute_reply":"2023-06-22T17:01:46.557508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport scipy.io\nimport numpy as np\nfrom scipy import signal\ndata_path = \"/kaggle/input/control1\"\nlowcut = 0.4 \nhighcut = 100 \nfs_original = 500\nfs_new = 250\n\ncontrol_data = []\ncontrol_path = os.path.join(data_path, 'Control')\n\nfor foldername in os.listdir(control_path):\n    subfolder_path = os.path.join(control_path, foldername)\n    control_data.append(subfolder_path)\n\nControl30 = []\n\nfor control_data_path in control_data:\n    n_epochs = 70 \n    start_epoch = 35\n    end_epoch = 70\n    control_arr = []\n\n    for i in range(start_epoch, end_epoch+1):\n        epoch_path = os.path.join(control_data_path, f\"trial{i}.mat\")\n        mat_data = scipy.io.loadmat(epoch_path)\n        mat_data1 = mat_data[\"trialData_i\"]        \n        # Downsampling\n        num_samples_original = mat_data1.shape[-1]\n        num_samples_new = int(num_samples_original * fs_new / fs_original)\n        downsampled_data = signal.resample(mat_data1, num_samples_new, axis=-1)\n        \n        control_arr.append(np.array(downsampled_data))\n        \n    Control30.append(np.array(control_arr))\n\nControl30 = np.array(Control30)\nprint(Control30[0].shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.560058Z","iopub.status.idle":"2023-06-22T17:01:46.560879Z","shell.execute_reply.started":"2023-06-22T17:01:46.560478Z","shell.execute_reply":"2023-06-22T17:01:46.560525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport json\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import entropy\n\ndelta_band = (0.5, 4)\nalpha_band = (8, 12)\nbeta_band = (12, 35)\n\nall_control_power = []\nscaler = MinMaxScaler()\nepsilon = 1e-10\n\n# Normalizing the features\nfor i in range(len(Control30)):\n    control_data = Control30[i]\n    control_power = []\n    for epoch in control_data:\n        epoch_power = []\n        for channel_data in epoch:\n            freq_spectrum = np.fft.fft(channel_data)\n            power_spectrum = np.abs(freq_spectrum) ** 2\n\n            alpha_power = np.sum(power_spectrum[(alpha_band[0] <= freq_spectrum) & (freq_spectrum <= alpha_band[1])])\n            beta_power = np.sum(power_spectrum[(beta_band[0] <= freq_spectrum) & (freq_spectrum <= beta_band[1])])\n            delta_power = np.sum(power_spectrum[(delta_band[0] <= freq_spectrum) & (freq_spectrum <= delta_band[1])])\n\n            channel_min_value = np.min(channel_data)\n            channel_max_value = np.max(channel_data)\n            power_spectrum_adjusted = power_spectrum + epsilon\n            entropy_values = entropy(power_spectrum_adjusted)\n            values=np.array([alpha_power,beta_power,delta_power,entropy_values])\n            rescaled_values = (values - values.min()) * (channel_max_value - channel_min_value) / (values.max() - values.min()) + channel_min_value\n            concatenated_data = np.concatenate((channel_data, rescaled_values))\n            epoch_power.append(concatenated_data.tolist())\n\n        control_power.append(np.array(epoch_power))\n\n    all_control_power.append(np.array(control_power))\n\nall_control_power = np.array(all_control_power)\nprint(all_control_power.shape)\nprint(len(Control30))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.563456Z","iopub.status.idle":"2023-06-22T17:01:46.564305Z","shell.execute_reply.started":"2023-06-22T17:01:46.563809Z","shell.execute_reply":"2023-06-22T17:01:46.563881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport scipy.io\nimport numpy as np\nfrom scipy import signal\ndata_path = \"/kaggle/input/concussed1\"\nlowcut = 0.4 \nhighcut = 100 \nfs_original = 500\nfs_new = 250  \n\nconcussed_data = []\nconcussed_path = os.path.join(data_path, 'Concussed')\nfor foldername in os.listdir(concussed_path):\n    subfolder_path = os.path.join(concussed_path, foldername)\n    concussed_data.append(subfolder_path)\n\nConcussed52 = []\nfor concussed_data_path in concussed_data:\n    n_epochs = 70\n    start_epoch = 35\n    end_epoch = 70\n    concussed_arr = []\n\n    for i in range(start_epoch, end_epoch+1):\n        epoch_path = f\"{concussed_data_path}/trial{i}.mat\"\n        mat_data = scipy.io.loadmat(epoch_path)\n        mat_data1 = mat_data[\"trialData_i\"]\n        num_samples_original = mat_data1.shape[-1]\n        num_samples_new = int(num_samples_original * fs_new / fs_original)\n        downsampled_data = signal.resample(mat_data1, num_samples_new, axis=-1)\n        \n        concussed_arr.append(np.array(downsampled_data))\n        \n    Concussed52.append(np.array(concussed_arr))\n\nConcussed52 = np.array(Concussed52)\nprint( Concussed52[0].shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.567046Z","iopub.status.idle":"2023-06-22T17:01:46.567831Z","shell.execute_reply.started":"2023-06-22T17:01:46.567445Z","shell.execute_reply":"2023-06-22T17:01:46.567474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport json\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import entropy\n\ndelta_band = (0.5, 4)\nalpha_band = (8, 12)\nbeta_band = (12, 35)\n\nall_concussed_power = []\nscaler = MinMaxScaler()\nepsilon = 1e-10\n\nfor i in range(len(Concussed52)):\n    concussed_data = Concussed52[i]\n    concussed_power = []\n    for epoch in concussed_data:\n        epoch_power = []\n        for channel_data in epoch:\n            freq_spectrum = np.fft.fft(channel_data)\n            power_spectrum = np.abs(freq_spectrum) ** 2\n\n            alpha_power = np.sum(power_spectrum[(alpha_band[0] <= freq_spectrum) & (freq_spectrum <= alpha_band[1])])\n            beta_power = np.sum(power_spectrum[(beta_band[0] <= freq_spectrum) & (freq_spectrum <= beta_band[1])])\n            delta_power = np.sum(power_spectrum[(delta_band[0] <= freq_spectrum) & (freq_spectrum <= delta_band[1])])\n\n            channel_min_value = np.min(channel_data)\n            channel_max_value = np.max(channel_data)\n            power_spectrum_adjusted = power_spectrum + epsilon\n            entropy_values = entropy(power_spectrum_adjusted)\n            values=np.array([alpha_power,beta_power,delta_power,entropy_values])\n            rescaled_values = (values - values.min()) * (channel_max_value - channel_min_value) / (values.max() - values.min()) + channel_min_value\n            concatenated_data = np.concatenate((channel_data, rescaled_values))\n            epoch_power.append(concatenated_data.tolist())\n\n        concussed_power.append(np.array(epoch_power))\n\n    all_concussed_power.append(np.array(concussed_power))\n\nall_concussed_power = np.array(all_concussed_power)\nprint(all_concussed_power.shape)\nprint(len(Concussed52))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.570693Z","iopub.status.idle":"2023-06-22T17:01:46.571640Z","shell.execute_reply.started":"2023-06-22T17:01:46.571203Z","shell.execute_reply":"2023-06-22T17:01:46.571253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\ndata_combined = np.concatenate((all_control_power, all_concussed_power), axis=0)\n\ncontrol_labels = np.zeros(all_control_power.shape[0])\nconcussed_labels = np.ones(all_concussed_power.shape[0])\nlabels_combined = np.concatenate((control_labels, concussed_labels))\n\nx_train_val, x_test, y_train_val, y_test = train_test_split(data_combined, labels_combined, test_size=0.2, stratify=labels_combined, random_state=42)\nx_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2, stratify=y_train_val, random_state=42)\nscaler = MinMaxScaler()\n\nx_train_flat = x_train.reshape(x_train.shape[0], -1)\nx_val_flat = x_val.reshape(x_val.shape[0], -1)\nx_test_flat = x_test.reshape(x_test.shape[0], -1)\nx_train_normalized = scaler.fit_transform(x_train_flat)\nx_val_normalized = scaler.transform(x_val_flat)\nx_test_normalized = scaler.transform(x_test_flat)\n\nx_train_normalized = x_train_normalized.reshape(x_train.shape)\nx_val_normalized = x_val_normalized.reshape(x_val.shape)\nx_test_normalized = x_test_normalized.reshape(x_test.shape)\nprint(\"x_train shape:\", x_train_normalized.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"x_val shape:\", x_val_normalized.shape)\nprint(\"y_val shape:\", y_val.shape)\nprint(\"x_test shape:\", x_test_normalized.shape)\nprint(\"y_test shape:\", y_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.575756Z","iopub.status.idle":"2023-06-22T17:01:46.576742Z","shell.execute_reply.started":"2023-06-22T17:01:46.576282Z","shell.execute_reply":"2023-06-22T17:01:46.576337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, GRU, Dense, Dropout, Bidirectional\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom keras import regularizers\n\nx_train_reshaped = x_train_normalized.reshape(x_train_normalized.shape[0], -1)\nx_test_reshaped = x_test_normalized.reshape(x_test_normalized.shape[0], -1)\n\nscaler = StandardScaler()\nx_train_scaled = scaler.fit_transform(x_train_reshaped)\nx_test_scaled = scaler.transform(x_test_reshaped)\ny_train_categorical = to_categorical(y_train)\ny_test_categorical = to_categorical(y_test)\n\nx_train_scaled = x_train_scaled.reshape(x_train_normalized.shape[0], x_train_normalized.shape[1], -1)\nx_test_scaled = x_test_scaled.reshape(x_test_normalized.shape[0], x_test_normalized.shape[1], -1)\nn_splits = 5\n\nkf = KFold(n_splits=n_splits, shuffle=True)\n\n# Lists to store evaluation results\nloss_scores = []\naccuracy_scores = []\nall_predictions = []\nall_true_labels = []\n\n# Perform k-fold cross-validation\nfor train_index, val_index in kf.split(x_train_scaled):\n    # Split the data into training and validation sets for this fold\n    x_train_fold, x_val_fold = x_train_scaled[train_index], x_train_scaled[val_index]\n    y_train_fold, y_val_fold = y_train_categorical[train_index], y_train_categorical[val_index]\n\n    # Build the model architecture with regularization\n    model = Sequential()\n    model.add(Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=regularizers.l2(0.01)),\n                            input_shape=(x_train_normalized.shape[1], x_train_scaled.shape[2])))\n    model.add(Dropout(0.5))\n    model.add(Bidirectional(LSTM(units=256, return_sequences=True, kernel_regularizer=regularizers.l2(0.01))))\n    model.add(Dropout(0.5))\n    model.add(Bidirectional(LSTM(units=128, return_sequences=True, kernel_regularizer=regularizers.l2(0.01))))\n    model.add(Dropout(0.5))\n    model.add(Bidirectional(GRU(units=64, return_sequences=True, kernel_regularizer=regularizers.l2(0.01))))\n    model.add(Dropout(0.5))\n    model.add(Bidirectional(GRU(units=32, kernel_regularizer=regularizers.l2(0.01))))\n    model.add(Dropout(0.5))\n    model.add(Dense(units=16, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n    model.add(Dense(units=2, activation='softmax'))\n\n    # Set the learning rate\n    learning_rate = 0.0006\n    optimizer = Adam(learning_rate=learning_rate)\n\n    # Compile the model\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n    # Define early stopping\n    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n\n    # Train the model with early stopping\n    history = model.fit(x_train_fold, y_train_fold, validation_data=(x_val_fold, y_val_fold),\n                        epochs=200, batch_size=128, callbacks=[early_stopping], verbose=0)\n\n    # Evaluate the model on the validation set\n    loss, accuracy = model.evaluate(x_val_fold, y_val_fold)\n    loss_scores.append(loss)\n    accuracy_scores.append(accuracy)\n    \n    val_predictions = model.predict(x_val_fold)\n    val_predicted_labels = np.argmax(val_predictions, axis=1)\n    val_true_labels = np.argmax(y_val_fold, axis=1)\n\n    # Append predictions and true labels to the lists\n    all_predictions.extend(val_predicted_labels)\n    all_true_labels.extend(val_true_labels)\n\n\n# Evaluate the model on the test set\nloss, accuracy = model.evaluate(x_test_scaled, y_test_categorical)\n\nprint('Test loss:', loss)\nprint('Test accuracy:', accuracy)\n\ntest_predictions = model.predict(x_test_scaled)\ntest_predicted_labels = np.argmax(test_predictions, axis=1)\ntest_true_labels = np.argmax(y_test_categorical, axis=1)\n\n# Calculate the confusion matrix\ncm_test = confusion_matrix(test_true_labels, test_predicted_labels)\nprint('Confusion Matrix (Test):')\nprint(cm_test)\n\n# Calculate the classification report\nclass_report_test = classification_report(test_true_labels, test_predicted_labels, target_names=['Control', 'Concussed'])\nprint('Classification Report (Test):')\nprint(class_report_test)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.579973Z","iopub.status.idle":"2023-06-22T17:01:46.581069Z","shell.execute_reply.started":"2023-06-22T17:01:46.580481Z","shell.execute_reply":"2023-06-22T17:01:46.580515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n\ny_pred_probs = model.predict(x_test_scaled)\n\ny_pred = np.argmax(y_pred_probs, axis=1)\n\ny_test_single = np.argmax(y_test_categorical, axis=1)\n\ncm = confusion_matrix(y_test_single, y_pred)\n\nreport = classification_report(y_test_single, y_pred)\n\nprint(\"Confusion Matrix:\")\nprint(cm)\nprint(\"\\nClassification Report:\")\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.584488Z","iopub.status.idle":"2023-06-22T17:01:46.586131Z","shell.execute_reply.started":"2023-06-22T17:01:46.585510Z","shell.execute_reply":"2023-06-22T17:01:46.585541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TP = 3\nFN = 3\nFP = 1\nTN = 9\n\nPrecision = TP / (TP + FP)\nSensitivity = TP / (TP + FN)\nSpecificity = TN /(TN + FP)\nFalse_Pos_Rate = FP / (TN + FP)\nFalse_Neg_Rate = FN / (FN + TP)\nF1_Score = (2*Precision*Sensitivity) / (Precision + Sensitivity)\nAccuracy = (TP+TN) / (TP+FP+TN+FN)\n\nprecision = \"{:.2f}\".format(Precision)\nsensitivity = \"{:.2f}\".format(Sensitivity)\nspecificity = \"{:.2f}\".format(Specificity)\nfalse_pos_rate = \"{:.2f}\".format(False_Pos_Rate)\nfalse_neg_rate = \"{:.2f}\".format(False_Neg_Rate)\nf1_score = \"{:.2f}\".format(F1_Score)\naccuracy = \"{:.2f}\".format(Accuracy)\n\nprint(\"Precision          :\", precision)\nprint(\"Sensitivity        :\", sensitivity)\nprint(\"Specificity        :\", specificity)\nprint(\"F1-Score           :\", f1_score)\nprint(\"False Positive Rate:\", false_pos_rate)\nprint(\"False Negative Rate:\", false_neg_rate)\nprint(\"Accuraccy          :\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.588967Z","iopub.status.idle":"2023-06-22T17:01:46.589829Z","shell.execute_reply.started":"2023-06-22T17:01:46.589473Z","shell.execute_reply":"2023-06-22T17:01:46.589513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tabulate import tabulate\n\ndata = [[\"Precision\", 0.75], \n        [\"Sensitivity\", 0.50], \n        [\"Specificity\", 0.90], \n        [\"F1-Score\", 0.60],\n        [\"False Positive Rate\", 0.10],\n        [\"False Negative Rate\", 0.50],\n        [\"Accuraccy\",0.75 ]]\n\ncol_names = [\"Performance Metrics\", \"Result\"]\nprint(tabulate(data, headers=col_names, tablefmt=\"fancy_grid\"))\n  \n     \n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.591987Z","iopub.status.idle":"2023-06-22T17:01:46.592793Z","shell.execute_reply.started":"2023-06-22T17:01:46.592409Z","shell.execute_reply":"2023-06-22T17:01:46.592440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cm.flatten()/np.sum(cm)]\n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\n\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in\n          zip(group_counts,group_percentages)]\n          \nlabels = np.asarray(labels).reshape(2,2)\n\nax = sns.heatmap(cm, fmt = '' ,annot=labels, cmap = 'Greens')\nax.set_title('Confusion Matrix\\n');\nax.set_xlabel('\\nPredicted Values')\nax.set_ylabel('\\nActual Values\\n');\n\nax.xaxis.set_ticklabels(['Control','Concussed'])\nax.yaxis.set_ticklabels(['Control','Concussed'])\n\nplt.show()\n     \n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T17:01:46.596535Z","iopub.status.idle":"2023-06-22T17:01:46.597210Z","shell.execute_reply.started":"2023-06-22T17:01:46.596849Z","shell.execute_reply":"2023-06-22T17:01:46.596895Z"},"trusted":true},"execution_count":null,"outputs":[]}]}