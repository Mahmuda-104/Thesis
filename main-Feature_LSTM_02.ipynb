{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport scipy.io\nimport numpy as np\nfrom scipy import signal\n\n# Set the path to the root directory where the \"Control\" folder is located\ndata_path = \"/kaggle/input/control1\"\nlowcut = 0.4 \nhighcut = 100 \nfs_original = 500\nfs_new = 250\n\ncontrol_data = []\ncontrol_path = os.path.join(data_path, 'Control')\n\nfor foldername in os.listdir(control_path):\n    subfolder_path = os.path.join(control_path, foldername)\n    control_data.append(subfolder_path)\n\nControl30 = []\n\nfor control_data_path in control_data:\n    n_epochs = 70 # Number of total epochs available\n    start_epoch = 35  # Starting epoch\n    end_epoch = 70  # Ending epoch\n    \n    control_arr = []\n\n    for i in range(start_epoch, end_epoch+1):\n        epoch_path = os.path.join(control_data_path, f\"trial{i}.mat\")\n        mat_data = scipy.io.loadmat(epoch_path)\n        mat_data1 = mat_data[\"trialData_i\"]        \n        # Downsampling\n        num_samples_original = mat_data1.shape[-1]\n        num_samples_new = int(num_samples_original * fs_new / fs_original)\n        downsampled_data = signal.resample(mat_data1, num_samples_new, axis=-1)\n        \n        control_arr.append(np.array(downsampled_data))\n        \n    Control30.append(np.array(control_arr))\n\nControl30 = np.array(Control30)\nprint(Control30[0].shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:05:31.951773Z","iopub.execute_input":"2023-06-19T05:05:31.952194Z","iopub.status.idle":"2023-06-19T05:05:50.116025Z","shell.execute_reply.started":"2023-06-19T05:05:31.952162Z","shell.execute_reply":"2023-06-19T05:05:50.114748Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"(36, 60, 1000)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport json\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import entropy\n\ndelta_band = (0.5, 4)\nalpha_band = (8, 12)\nbeta_band = (12, 35)\n\nall_control_power = []\nscaler = MinMaxScaler()\nepsilon = 1e-10\n\n# Normalizing the features\nfor i in range(len(Control30)):\n    control_data = Control30[i]\n    control_power = []\n    for epoch in control_data:\n        epoch_power = []\n        for channel_data in epoch:\n            freq_spectrum = np.fft.fft(channel_data)\n            power_spectrum = np.abs(freq_spectrum) ** 2\n\n            alpha_power = np.sum(power_spectrum[(alpha_band[0] <= freq_spectrum) & (freq_spectrum <= alpha_band[1])])\n            beta_power = np.sum(power_spectrum[(beta_band[0] <= freq_spectrum) & (freq_spectrum <= beta_band[1])])\n            delta_power = np.sum(power_spectrum[(delta_band[0] <= freq_spectrum) & (freq_spectrum <= delta_band[1])])\n\n            channel_min_value = np.min(channel_data)\n            channel_max_value = np.max(channel_data)\n\n            # alpha_power_normalized = (alpha_power - channel_min_value) / (4*((channel_max_value-channel_min_value) * (channel_max_value-channel_min_value )))\n            # beta_power_normalized = (beta_power - channel_min_value) /  (4*((channel_max_value-channel_min_value) * (channel_max_value-channel_min_value) ))\n            # delta_power_normalized = (delta_power - channel_min_value) /  (4*((channel_max_value-channel_min_value) * (channel_max_value-channel_min_value)))\n\n            power_spectrum_adjusted = power_spectrum + epsilon\n            entropy_values = entropy(power_spectrum_adjusted)\n            values=np.array([alpha_power,beta_power,delta_power,entropy_values])\n            rescaled_values = (values - values.min()) * (channel_max_value - channel_min_value) / (values.max() - values.min()) + channel_min_value\n            concatenated_data = np.concatenate((channel_data, rescaled_values))\n\n            #concatenated_data = np.concatenate((channel_data, [alpha_power_normalized, beta_power_normalized, delta_power_normalized, entropy_values]))\n            #print(alpha_power_normalized, beta_power_normalized, delta_power_normalized, entropy_values)\n            epoch_power.append(concatenated_data.tolist())\n\n        control_power.append(np.array(epoch_power))\n\n    all_control_power.append(np.array(control_power))\n\nall_control_power = np.array(all_control_power)\nprint(all_control_power.shape)\nprint(len(Control30))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:05:50.119604Z","iopub.execute_input":"2023-06-19T05:05:50.120010Z","iopub.status.idle":"2023-06-19T05:06:18.783707Z","shell.execute_reply.started":"2023-06-19T05:05:50.119979Z","shell.execute_reply":"2023-06-19T05:06:18.782403Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"(30, 36, 60, 1004)\n30\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport scipy.io\nimport numpy as np\nfrom scipy import signal\ndata_path = \"/kaggle/input/concussed1\"\nlowcut = 0.4 \nhighcut = 100 \nfs_original = 500\nfs_new = 250  \n\nconcussed_data = []\nconcussed_path = os.path.join(data_path, 'Concussed')\nfor foldername in os.listdir(concussed_path):\n    subfolder_path = os.path.join(concussed_path, foldername)\n    concussed_data.append(subfolder_path)\n\nConcussed52 = []\nfor concussed_data_path in concussed_data:\n    n_epochs = 70 # Number of total epochs available\n    start_epoch = 35  # Starting epoch\n    end_epoch = 70  # Ending epoch\n    \n    concussed_arr = []\n\n    for i in range(start_epoch, end_epoch+1):\n        epoch_path = f\"{concussed_data_path}/trial{i}.mat\"\n        mat_data = scipy.io.loadmat(epoch_path)\n        mat_data1 = mat_data[\"trialData_i\"]\n        num_samples_original = mat_data1.shape[-1]\n        num_samples_new = int(num_samples_original * fs_new / fs_original)\n        downsampled_data = signal.resample(mat_data1, num_samples_new, axis=-1)\n        \n        concussed_arr.append(np.array(downsampled_data))\n        \n    Concussed52.append(np.array(concussed_arr))\n\nConcussed52 = np.array(Concussed52)\nprint( Concussed52[0].shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:06:18.785555Z","iopub.execute_input":"2023-06-19T05:06:18.785944Z","iopub.status.idle":"2023-06-19T05:06:45.225316Z","shell.execute_reply.started":"2023-06-19T05:06:18.785909Z","shell.execute_reply":"2023-06-19T05:06:45.224032Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(36, 60, 1000)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport json\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import entropy\n\ndelta_band = (0.5, 4)\nalpha_band = (8, 12)\nbeta_band = (12, 35)\n\nall_concussed_power = []\nscaler = MinMaxScaler()\nepsilon = 1e-10\n\n# Normalizing the features within the range of the minimum and maximum values of the corresponding channel\nfor i in range(len(Concussed52)):\n    concussed_data = Concussed52[i]\n    concussed_power = []\n    for epoch in concussed_data:\n        epoch_power = []\n        for channel_data in epoch:\n            freq_spectrum = np.fft.fft(channel_data)\n            power_spectrum = np.abs(freq_spectrum) ** 2\n\n            alpha_power = np.sum(power_spectrum[(alpha_band[0] <= freq_spectrum) & (freq_spectrum <= alpha_band[1])])\n            beta_power = np.sum(power_spectrum[(beta_band[0] <= freq_spectrum) & (freq_spectrum <= beta_band[1])])\n            delta_power = np.sum(power_spectrum[(delta_band[0] <= freq_spectrum) & (freq_spectrum <= delta_band[1])])\n\n            channel_min_value = np.min(channel_data)\n            channel_max_value = np.max(channel_data)\n\n            # alpha_power_normalized = (alpha_power - channel_min_value) / (4*((channel_max_value-channel_min_value) * (channel_max_value-channel_min_value )))\n            # beta_power_normalized = (beta_power - channel_min_value) /  (4*((channel_max_value-channel_min_value) * (channel_max_value-channel_min_value) ))\n            # delta_power_normalized = (delta_power - channel_min_value) /  (4*((channel_max_value-channel_min_value) * (channel_max_value-channel_min_value)))\n\n            power_spectrum_adjusted = power_spectrum + epsilon\n            entropy_values = entropy(power_spectrum_adjusted)\n            values=np.array([alpha_power,beta_power,delta_power,entropy_values])\n            rescaled_values = (values - values.min()) * (channel_max_value - channel_min_value) / (values.max() - values.min()) + channel_min_value\n            concatenated_data = np.concatenate((channel_data, rescaled_values))\n\n            #concatenated_data = np.concatenate((channel_data, [alpha_power_normalized, beta_power_normalized, delta_power_normalized, entropy_values]))\n            #print(alpha_power_normalized, beta_power_normalized, delta_power_normalized, entropy_values)\n            epoch_power.append(concatenated_data.tolist())\n\n        concussed_power.append(np.array(epoch_power))\n\n    all_concussed_power.append(np.array(concussed_power))\n\nall_concussed_power = np.array(all_concussed_power)\nprint(all_concussed_power.shape)\nprint(len(Concussed52))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:06:45.229554Z","iopub.execute_input":"2023-06-19T05:06:45.230210Z","iopub.status.idle":"2023-06-19T05:07:32.107591Z","shell.execute_reply.started":"2023-06-19T05:06:45.230175Z","shell.execute_reply":"2023-06-19T05:07:32.106647Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"(49, 36, 60, 1004)\n49\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Step 1: Data Preparation\n# Combine control and concussed data\ndata_combined = np.concatenate((all_control_power, all_concussed_power), axis=0)\n\n# Create labels for binary classification\ncontrol_labels = np.zeros(all_control_power.shape[0])\nconcussed_labels = np.ones(all_concussed_power.shape[0])\nlabels_combined = np.concatenate((control_labels, concussed_labels))\n\n# Step 2: Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(data_combined, labels_combined, test_size=0.2, random_state=42)\n\n# Step 3: Normalize the data\nscaler = MinMaxScaler()\nx_train_normalized = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\nx_test_normalized = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n\n# Reshape the normalized data back to its original shape\nx_train_normalized = x_train_normalized.reshape(x_train.shape)\nx_test_normalized = x_test_normalized.reshape(x_test.shape)\n\n# Verify the shapes of the training and testing sets\nprint(\"x_train shape:\", x_train_normalized.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"x_test shape:\", x_test_normalized.shape)\nprint(\"y_test shape:\", y_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:07:32.108755Z","iopub.execute_input":"2023-06-19T05:07:32.109358Z","iopub.status.idle":"2023-06-19T05:07:35.242423Z","shell.execute_reply.started":"2023-06-19T05:07:32.109326Z","shell.execute_reply":"2023-06-19T05:07:35.241257Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"x_train shape: (63, 36, 60, 1004)\ny_train shape: (63,)\nx_test shape: (16, 36, 60, 1004)\ny_test shape: (16,)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, GRU, Dense, Dropout, Bidirectional\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n\n# Reshape the data\nx_train_reshaped = x_train.reshape(x_train.shape[0], -1)\nx_test_reshaped = x_test.reshape(x_test.shape[0], -1)\n\n# Standardize the data\nscaler = StandardScaler()\nx_train_scaled = scaler.fit_transform(x_train_reshaped)\nx_test_scaled = scaler.transform(x_test_reshaped)\n\n# Convert the labels to categorical (one-hot encoding)\ny_train_categorical = to_categorical(y_train)\ny_test_categorical = to_categorical(y_test)\n\n# Reshape the flattened data back to 3D shape\nx_train_scaled = x_train_scaled.reshape(x_train.shape[0], x_train.shape[1], -1)\nx_test_scaled = x_test_scaled.reshape(x_test.shape[0], x_test.shape[1], -1)\n\n# Define the number of folds\nn_splits = 4\n\n# Create a KFold object\nkf = KFold(n_splits=n_splits, shuffle=True)\n\n# Lists to store evaluation results\nloss_scores = []\naccuracy_scores = []\nall_predictions = []\nall_true_labels = []\n\n\n# Perform k-fold cross-validation\nfor train_index, val_index in kf.split(x_train_scaled):\n    # Split the data into training and validation sets for this fold\n    x_train_fold, x_val_fold = x_train_scaled[train_index], x_train_scaled[val_index]\n    y_train_fold, y_val_fold = y_train_categorical[train_index], y_train_categorical[val_index]\n\n    # Build the model architecture\n    model = Sequential()\n    model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(x_train.shape[1], x_train_scaled.shape[2])))\n    model.add(Dropout(0.5))\n    model.add(Bidirectional(LSTM(units=256, return_sequences=True)))\n    model.add(Dropout(0.5))\n    model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n    model.add(Dropout(0.5))\n    model.add(Bidirectional(GRU(units=64, return_sequences=True)))\n    model.add(Dropout(0.5))\n    model.add(Bidirectional(GRU(units=32)))\n    model.add(Dropout(0.5))\n    model.add(Dense(units=16, activation='relu'))\n    model.add(Dense(units=2, activation='softmax'))\n\n    # Set the learning rate\n    learning_rate = 0.0006\n    optimizer = Adam(learning_rate=learning_rate)\n\n    # Compile the model\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n    # Define early stopping\n    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n\n    # Train the model with early stopping\n    history = model.fit(x_train_fold, y_train_fold, validation_data=(x_val_fold, y_val_fold),\n                        epochs=200, batch_size=128, callbacks=[early_stopping], verbose=0)\n\n    # Evaluate the model on the validation set\n    loss, accuracy = model.evaluate(x_val_fold, y_val_fold)\n    loss_scores.append(loss)\n    accuracy_scores.append(accuracy)\n    \n    val_predictions = model.predict(x_val_fold)\n    val_predicted_labels = np.argmax(val_predictions, axis=1)\n    val_true_labels = np.argmax(y_val_fold, axis=1)\n\n    # Append predictions and true labels to the lists\n    all_predictions.extend(val_predicted_labels)\n    all_true_labels.extend(val_true_labels)\n\n\n# Calculate the average scores across all folds\navg_loss = np.mean(loss_scores)\navg_accuracy = np.mean(accuracy_scores)\n\nprint('Average validation loss:', avg_loss)\nprint('Average validation accuracy:', avg_accuracy)\n\n# Calculate the confusion matrix\ncm = confusion_matrix(all_true_labels, all_predictions)\nprint('Confusion Matrix:')\nprint(cm)\n\n# Calculate the classification report\nclass_report = classification_report(all_true_labels, all_predictions)\nprint('Classification Report:')\nprint(class_report)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:07:35.244092Z","iopub.execute_input":"2023-06-19T05:07:35.244536Z","iopub.status.idle":"2023-06-19T05:29:33.399588Z","shell.execute_reply.started":"2023-06-19T05:07:35.244507Z","shell.execute_reply":"2023-06-19T05:29:33.398124Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 2s 2s/step - loss: 0.5022 - accuracy: 0.6875\n1/1 [==============================] - 6s 6s/step\n1/1 [==============================] - 2s 2s/step - loss: 0.5406 - accuracy: 0.8125\n1/1 [==============================] - 5s 5s/step\n1/1 [==============================] - 2s 2s/step - loss: 0.5644 - accuracy: 0.8125\n1/1 [==============================] - 5s 5s/step\n1/1 [==============================] - 1s 1s/step - loss: 0.6324 - accuracy: 0.5333\n1/1 [==============================] - 5s 5s/step\nAverage validation loss: 0.5599031150341034\nAverage validation accuracy: 0.7114583402872086\nConfusion Matrix:\n[[10 13]\n [ 5 35]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.67      0.43      0.53        23\n           1       0.73      0.88      0.80        40\n\n    accuracy                           0.71        63\n   macro avg       0.70      0.65      0.66        63\nweighted avg       0.71      0.71      0.70        63\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n\ny_pred_probs = model.predict(x_test_scaled)\n\ny_pred = np.argmax(y_pred_probs, axis=1)\n\ny_test_single = np.argmax(y_test_categorical, axis=1)\n\ncm = confusion_matrix(y_test_single, y_pred)\n\nreport = classification_report(y_test_single, y_pred)\n\nprint(\"Confusion Matrix:\")\nprint(cm)\nprint(\"\\nClassification Report:\")\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:41:39.710586Z","iopub.execute_input":"2023-06-19T05:41:39.711163Z","iopub.status.idle":"2023-06-19T05:41:42.741784Z","shell.execute_reply.started":"2023-06-19T05:41:39.711127Z","shell.execute_reply":"2023-06-19T05:41:42.739041Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 3s 3s/step\nConfusion Matrix:\n[[3 4]\n [0 9]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      0.43      0.60         7\n           1       0.69      1.00      0.82         9\n\n    accuracy                           0.75        16\n   macro avg       0.85      0.71      0.71        16\nweighted avg       0.83      0.75      0.72        16\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.regularizers import l2\nimport numpy as np\n\n# Reshape the input data\nX_train = np.reshape(X_train, (X_train.shape[0]*X_train.shape[1], X_train.shape[2], X_train.shape[3]))\nX_val = np.reshape(X_val, (X_val.shape[0]*X_val.shape[1], X_val.shape[2], X_val.shape[3]))\n\n# Build the model\ninput_shape = (X_train.shape[1], X_train.shape[2])\n\nmodel = tf.keras.Sequential()\nmodel.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]*X.shape[3]), return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.0001)))\nmodel.add(layers.Dropout(0.5))\nmodel.add(GRU(256, kernel_regularizer=tf.keras.regularizers.l2(0.0001), return_sequences=True))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:35.170242Z","iopub.execute_input":"2023-06-19T05:29:35.170791Z","iopub.status.idle":"2023-06-19T05:29:38.060541Z","shell.execute_reply.started":"2023-06-19T05:29:35.170760Z","shell.execute_reply":"2023-06-19T05:29:38.055280Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Reshape the input data\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(\u001b[43mX_train\u001b[49m, (X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]))\n\u001b[1;32m      8\u001b[0m X_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(X_val, (X_val\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mX_val\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], X_val\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], X_val\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Build the model\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"],"ename":"NameError","evalue":"name 'X_train' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\n\nX = dataset\ny = labels\nEpoch = 100\nbatchSize = 256\n\nk_folds = 2\nkf = KFold(n_splits=k_folds, shuffle=True)\n\ntrain_accuracies = []\nval_accuracies = []\ncm_total = np.zeros((2, 2))\ny_preds = []\n\nmodel = tf.keras.Sequential()\nmodel.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True), input_shape=(X.shape[1], X.shape[2]*X.shape[3])))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.GRU(256, return_sequences=True))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.GRU(64, return_sequences=True))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Bidirectional(layers.GRU(64, return_sequences=True)))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.GRU(32))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(2, activation='softmax'))\n\nmodel.compile(optimizer=\"adam\", loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[\"accuracy\"])\nmodel.summary()\n\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\ny_categorical = to_categorical(y_encoded)\n\nfor train_index, val_index in kf.split(X):\n    X_train, X_val = X[train_index], X[val_index]\n    y_train, y_val = y_categorical[train_index], y_categorical[val_index]\n\n    model.reset_states()\n\n    history = model.fit(\n        X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2]*X_train.shape[3]),\n        y_train,\n        epochs=Epoch,\n        batch_size=batchSize,\n        validation_data=(X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2]*X_val.shape[3]), y_val),\n        verbose=1\n    )\n\n    train_loss, train_accuracy = model.evaluate(X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2]*X_train.shape[3]), y_train)\n    val_loss, val_accuracy = model.evaluate(X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2]*X_val.shape[3]), y_val)\n\n    train_accuracies.append(train_accuracy)\n    val_accuracies.append(val_accuracy)\n\n    y_pred = model.predict(X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2]*X_val.shape[3]))\n    y_pred = np.argmax(y_pred, axis=1)\n    y_preds.append(y_pred)\n\n    cm = confusion_matrix(np.argmax(y_val, axis=1), y_pred)\n    cm_total += cm\n\navg_train_accuracy = np.mean(train_accuracies)\navg_val_accuracy = np.mean(val_accuracies)\ncm_avg = cm_total / k_folds\n\ny_preds = np.concatenate(y_preds)\n\nprint('Confusion Matrix')\nprint(cm_total)\nprint('\\n')\n\n# Define target names\ntarget_names = ['Control', 'Concussed']\n\n# Calculate classification report\nprint('Classification Report')\nprint(classification_report(y_encoded, y_preds, target_names=target_names))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.061535Z","iopub.status.idle":"2023-06-19T05:29:38.062003Z","shell.execute_reply.started":"2023-06-19T05:29:38.061772Z","shell.execute_reply":"2023-06-19T05:29:38.061793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_train_accuracy = np.mean(train_accuracies)\navg_val_accuracy = np.mean(val_accuracies)\n\nprint('Average Training Accuracy:', avg_train_accuracy)\nprint('Average Validation Accuracy:', avg_val_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.063966Z","iopub.status.idle":"2023-06-19T05:29:38.064373Z","shell.execute_reply.started":"2023-06-19T05:29:38.064183Z","shell.execute_reply":"2023-06-19T05:29:38.064201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Confusion Matrix')\nprint(cm_total)\nprint('\\n')\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.065660Z","iopub.status.idle":"2023-06-19T05:29:38.066097Z","shell.execute_reply.started":"2023-06-19T05:29:38.065880Z","shell.execute_reply":"2023-06-19T05:29:38.065906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gjfdsl","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.067496Z","iopub.status.idle":"2023-06-19T05:29:38.067937Z","shell.execute_reply.started":"2023-06-19T05:29:38.067702Z","shell.execute_reply":"2023-06-19T05:29:38.067721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport scipy.io\nimport numpy as np\nfrom scipy import signal\n\n# Set the path to the root directory where the \"Control\" folder is located\ndata_path = \"/kaggle/input/control1\"\nlowcut = 0.4 \nhighcut = 50 \nfs_original = 500\nfs_new = 250\n\ncontrol_data = []\ncontrol_path = os.path.join(data_path, 'Control')\n\nfor foldername in os.listdir(control_path):\n    subfolder_path = os.path.join(control_path, foldername)\n    control_data.append(subfolder_path)\n\nControl30 = []\n\nfor control_data_path in control_data:\n    n_epochs = 105#len([f for f in os.listdir(control_data_path) if f.endswith('.mat')])\n    control_arr = []\n    \n    for i in range(1, n_epochs+1):\n        epoch_path = os.path.join(control_data_path, f\"trial{i}.mat\")\n        mat_data = scipy.io.loadmat(epoch_path)\n        mat_data1 = mat_data[\"trialData_i\"]\n                # Average referencing\n#         average_potential = np.mean(mat_data1, axis=0, keepdims=True)\n#         referenced_data = mat_data1 - average_potential\n        \n        # Bandpass filtering\n        b, a = signal.butter(4, [lowcut, highcut], fs=fs_original, btype='band')\n        filtered_data = signal.filtfilt(b, a, mat_data1, axis=-1)\n        \n        # Apply notch filter\n        f0 = 60  # Frequency to be removed (e.g., power line interference)\n        Q = 30   # Quality factor\n        w0 = f0 / (fs_original / 2)\n        b, a = signal.iirnotch(w0, Q)\n        filtered_data = signal.filtfilt(b, a, filtered_data, axis=-1)\n\n        \n        # Downsampling\n        num_samples_original = mat_data1.shape[-1]\n        num_samples_new = int(num_samples_original * fs_new / fs_original)\n        downsampled_data = signal.resample(mat_data1, num_samples_new, axis=-1)\n        \n        control_arr.append(np.array(downsampled_data))\n        \n    Control30.append(np.array(control_arr))\n\nControl30 = np.array(Control30)\nprint(Control30[0].shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.069753Z","iopub.status.idle":"2023-06-19T05:29:38.070207Z","shell.execute_reply.started":"2023-06-19T05:29:38.070011Z","shell.execute_reply":"2023-06-19T05:29:38.070030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n# Reshape the Control30 data to prepare for scaling\nnum_trials, num_epochs, num_channels, num_samples = Control30.shape\n\nControl30_scaled = np.zeros_like(Control30)\n\nscaler = MinMaxScaler()\n\nfor i in range(num_trials):\n    for j in range(num_epochs):\n        # Flatten the 2D EEG data for scaling\n        data = Control30[i, j].reshape(num_channels, num_samples)\n\n        # Apply min-max scaling\n        scaled_data = scaler.fit_transform(data)\n\n        # Reshape the scaled data back to 2D\n        Control30_scaled[i, j] = scaled_data.reshape(num_channels, num_samples)\n\nprint(Control30_scaled[0].shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.072882Z","iopub.status.idle":"2023-06-19T05:29:38.073298Z","shell.execute_reply.started":"2023-06-19T05:29:38.073104Z","shell.execute_reply":"2023-06-19T05:29:38.073124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import json\n# from sklearn.preprocessing import MinMaxScaler\n# from scipy.stats import entropy\n\n# delta_band = (0.5, 4)\n# alpha_band = (8, 12)\n# beta_band = (12, 35)\n\n# all_control_power = []\n# scaler = MinMaxScaler()\n# epsilon = 1e-10\n\n# # Normalizing the features\n# for i in range(len(Control30)):\n#     control_data = Control30[i]\n#     control_power = []\n#     for epoch in control_data:\n#         epoch_power = []\n#         for channel_data in epoch:\n#             freq_spectrum = np.fft.fft(channel_data)\n#             power_spectrum = np.abs(freq_spectrum) ** 2\n\n#             alpha_power = np.sum(power_spectrum[(alpha_band[0] <= freq_spectrum) & (freq_spectrum <= alpha_band[1])])\n#             beta_power = np.sum(power_spectrum[(beta_band[0] <= freq_spectrum) & (freq_spectrum <= beta_band[1])])\n#             delta_power = np.sum(power_spectrum[(delta_band[0] <= freq_spectrum) & (freq_spectrum <= delta_band[1])])\n\n#             channel_min_value = np.min(channel_data)\n#             channel_max_value = np.max(channel_data)\n\n#             # alpha_power_normalized = (alpha_power - channel_min_value) / (4*((channel_max_value-channel_min_value) * (channel_max_value-channel_min_value )))\n#             # beta_power_normalized = (beta_power - channel_min_value) /  (4*((channel_max_value-channel_min_value) * (channel_max_value-channel_min_value) ))\n#             # delta_power_normalized = (delta_power - channel_min_value) /  (4*((channel_max_value-channel_min_value) * (channel_max_value-channel_min_value)))\n\n#             power_spectrum_adjusted = power_spectrum + epsilon\n#             entropy_values = entropy(power_spectrum_adjusted)\n#             values=np.array([alpha_power,beta_power,delta_power,entropy_values])\n#             rescaled_values = (values - values.min()) * (channel_max_value - channel_min_value) / (values.max() - values.min()) + channel_min_value\n#             concatenated_data = np.concatenate((channel_data, rescaled_values))\n\n#             #concatenated_data = np.concatenate((channel_data, [alpha_power_normalized, beta_power_normalized, delta_power_normalized, entropy_values]))\n#             #print(alpha_power_normalized, beta_power_normalized, delta_power_normalized, entropy_values)\n#             epoch_power.append(concatenated_data.tolist())\n\n#         control_power.append(np.array(epoch_power))\n\n#     all_control_power.append(np.array(control_power))\n\n# all_control_power = np.array(all_control_power)\n# print(all_control_power.shape)\n# print(len(Control30))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.075178Z","iopub.status.idle":"2023-06-19T05:29:38.075587Z","shell.execute_reply.started":"2023-06-19T05:29:38.075388Z","shell.execute_reply":"2023-06-19T05:29:38.075407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport scipy.io\nimport numpy as np\nfrom scipy import signal\ndata_path = \"/kaggle/input/concussed1\"\nlowcut = 0.4 \nhighcut = 50 \nfs_original = 500\nfs_new = 250  \n\nconcussed_data = []\nconcussed_path = os.path.join(data_path, 'Concussed')\nfor foldername in os.listdir(concussed_path):\n    subfolder_path = os.path.join(concussed_path, foldername)\n    concussed_data.append(subfolder_path)\n\nConcussed52 = []\nfor concussed_data_path in concussed_data:\n    n_epochs = 105 #len([f for f in os.listdir(control_data_path) if f.endswith('.mat')])\n    concussed_arr = []\n    for i in range(1, n_epochs+1):\n        epoch_path = f\"{concussed_data_path}/trial{i}.mat\"\n        mat_data = scipy.io.loadmat(epoch_path)\n        mat_data1 = mat_data[\"trialData_i\"]\n#         # Average referencing\n#         average_potential = np.mean(mat_data1, axis=0, keepdims=True)\n#         referenced_data = mat_data1 - average_potential\n        \n        # Bandpass filtering\n        b, a = signal.butter(4, [lowcut, highcut], fs=fs_original, btype='band')\n        filtered_data = signal.filtfilt(b, a, mat_data1, axis=-1)\n        \n        # Apply notch filter\n        f0 = 60  # Frequency to be removed (e.g., power line interference)\n        Q = 30   # Quality factor\n        w0 = f0 / (fs_original / 2)\n        b, a = signal.iirnotch(w0, Q)\n        filtered_data = signal.filtfilt(b, a, filtered_data, axis=-1)\n        \n        # Downsampling\n        num_samples_original = mat_data1.shape[-1]\n        num_samples_new = int(num_samples_original * fs_new / fs_original)\n        downsampled_data = signal.resample(mat_data1, num_samples_new, axis=-1)\n        \n        concussed_arr.append(np.array(downsampled_data))\n        \n    Concussed52.append(np.array(concussed_arr))\n\nConcussed52 = np.array(Concussed52)\nprint( Concussed52[0].shape)\n\nConcussed52 = Concussed52[:30]\nprint(Concussed52.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.078336Z","iopub.status.idle":"2023-06-19T05:29:38.078749Z","shell.execute_reply.started":"2023-06-19T05:29:38.078551Z","shell.execute_reply":"2023-06-19T05:29:38.078570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n# Reshape the Control30 data to prepare for scaling\nnum_trials, num_epochs, num_channels, num_samples = Concussed52.shape\n\nConcussed52_scaled = np.zeros_like(Concussed52)\n\nscaler = MinMaxScaler()\n\nfor i in range(num_trials):\n    for j in range(num_epochs):\n        # Flatten the 2D EEG data for scaling\n        data = Concussed52[i, j].reshape(num_channels, num_samples)\n\n        # Apply min-max scaling\n        scaled_data = scaler.fit_transform(data)\n\n        # Reshape the scaled data back to 2D\n        Concussed52_scaled[i, j] = scaled_data.reshape(num_channels, num_samples)\n\nprint(Concussed52_scaled[0].shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.080821Z","iopub.status.idle":"2023-06-19T05:29:38.081981Z","shell.execute_reply.started":"2023-06-19T05:29:38.081735Z","shell.execute_reply":"2023-06-19T05:29:38.081756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import json\n# from sklearn.preprocessing import MinMaxScaler\n\n# from scipy.stats import entropy\n\n# delta_band = (0.5, 4)\n# alpha_band = (8, 12)\n# beta_band = (12, 35)\n\n# all_concussed_power = []\n# scaler = MinMaxScaler()\n# epsilon = 1e-10\n\n# # Normalizing the features within the range of the minimum and maximum values of the corresponding channel\n# for i in range(len(Concussed52)):\n#     concussed_data = Concussed52[i]\n#     concussed_power = []\n#     for epoch in concussed_data:\n#         epoch_power = []\n#         for channel_data in epoch:\n#             freq_spectrum = np.fft.fft(channel_data)\n#             power_spectrum = np.abs(freq_spectrum) ** 2\n\n#             alpha_power = np.sum(power_spectrum[(alpha_band[0] <= freq_spectrum) & (freq_spectrum <= alpha_band[1])])\n#             beta_power = np.sum(power_spectrum[(beta_band[0] <= freq_spectrum) & (freq_spectrum <= beta_band[1])])\n#             delta_power = np.sum(power_spectrum[(delta_band[0] <= freq_spectrum) & (freq_spectrum <= delta_band[1])])\n\n#             channel_min_value = np.min(channel_data)\n#             channel_max_value = np.max(channel_data)\n\n#             # alpha_power_normalized = (alpha_power - channel_min_value) / (4*((channel_max_value-channel_min_value) * (channel_max_value-channel_min_value )))\n#             # beta_power_normalized = (beta_power - channel_min_value) /  (4*((channel_max_value-channel_min_value) * (channel_max_value-channel_min_value) ))\n#             # delta_power_normalized = (delta_power - channel_min_value) /  (4*((channel_max_value-channel_min_value) * (channel_max_value-channel_min_value)))\n\n#             power_spectrum_adjusted = power_spectrum + epsilon\n#             entropy_values = entropy(power_spectrum_adjusted)\n#             values=np.array([alpha_power,beta_power,delta_power,entropy_values])\n#             rescaled_values = (values - values.min()) * (channel_max_value - channel_min_value) / (values.max() - values.min()) + channel_min_value\n#             concatenated_data = np.concatenate((channel_data, rescaled_values))\n\n#             #concatenated_data = np.concatenate((channel_data, [alpha_power_normalized, beta_power_normalized, delta_power_normalized, entropy_values]))\n#             #print(alpha_power_normalized, beta_power_normalized, delta_power_normalized, entropy_values)\n#             epoch_power.append(concatenated_data.tolist())\n\n#         concussed_power.append(np.array(epoch_power))\n\n#     all_concussed_power.append(np.array(concussed_power))\n\n# all_concussed_power = np.array(all_concussed_power)\n# print(all_concussed_power.shape)\n# print(len(Concussed52))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.083573Z","iopub.status.idle":"2023-06-19T05:29:38.084033Z","shell.execute_reply.started":"2023-06-19T05:29:38.083791Z","shell.execute_reply":"2023-06-19T05:29:38.083813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = np.concatenate((Control30_scaled, Concussed52_scaled), axis=0)\nlabels = np.concatenate((np.zeros(len(Control30_scaled)), np.ones(len(Concussed52_scaled))))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.087952Z","iopub.status.idle":"2023-06-19T05:29:38.089021Z","shell.execute_reply.started":"2023-06-19T05:29:38.088697Z","shell.execute_reply":"2023-06-19T05:29:38.088718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Bidirectional, LSTM, Dropout, GRU, Dense, Activation\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, random_state=42)\nprint(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_test shape:\", y_test.shape)\n\n# Split the training data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\nprint(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_val shape:\", X_val.shape)\nprint(\"y_val shape:\", y_val.shape)\n\n# Reshape the data to match the expected input shape of the model\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], -1))\nX_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], -1))\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], -1))\n\n# Define the input shape based on the reshaped data\ninput_shape = (X_train.shape[1], X_train.shape[2])\n\n# Define the number of classes\nnum_classes = 2  # Assuming binary classification\n\n# Convert the target variables to categorical format\ny_train_categorical = tf.keras.utils.to_categorical(y_train, num_classes)\ny_val_categorical = tf.keras.utils.to_categorical(y_val, num_classes)\ny_test_categorical = tf.keras.utils.to_categorical(y_test, num_classes)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.090284Z","iopub.status.idle":"2023-06-19T05:29:38.090931Z","shell.execute_reply.started":"2023-06-19T05:29:38.090634Z","shell.execute_reply":"2023-06-19T05:29:38.090653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Build the model\nmodel = Sequential()\n\nmodel.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=input_shape))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units=256, return_sequences=True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units=64, return_sequences=True))\nmodel.add(Dropout(0.2))\n\nmodel.add(Bidirectional(GRU(units=64, return_sequences=True)))\nmodel.add(Dropout(0.2))\n\nmodel.add(GRU(units=32))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units=16))\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(units=num_classes))\nmodel.add(Activation('softmax'))\n\n# Compile the model\noptimizer = Adam(learning_rate=0.005)\nmodel.compile(optimizer=optimizer, loss=tf.keras.losses.categorical_crossentropy, metrics=[\"accuracy\"])\nmodel.summary()\n\n# # Train the model\n# batch_size = 128\n# epochs = 20\n# model.fit(X_train, y_train_categorical, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val_categorical))\n\n# # Evaluate the model on the testing data\n# score = model.evaluate(X_test, y_test_categorical, verbose=0)\n# print(\"Testing loss:\", score[0])\n# print(\"Testing accuracy:\", score[1])\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.092908Z","iopub.status.idle":"2023-06-19T05:29:38.093881Z","shell.execute_reply.started":"2023-06-19T05:29:38.093651Z","shell.execute_reply":"2023-06-19T05:29:38.093672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\n# Amplitude scaling augmentation function\ndef amplitude_scaling(data, factor_range=(0.8, 1.2)):\n    scaling_factor = random.uniform(factor_range[0], factor_range[1])\n    return data * scaling_factor\n\n# Random noise addition augmentation function\ndef add_random_noise(data, noise_range=(-0.001, 0.001)):\n    noise = np.random.uniform(noise_range[0], noise_range[1], size=data.shape)\n    return data + noise\n\n# Apply augmentation to the training data\nX_train_augmented = []\nfor sample in X_train:\n    scaled_sample = amplitude_scaling(sample)\n    noisy_sample = add_random_noise(scaled_sample)\n    X_train_augmented.append(noisy_sample)\nX_train_augmented = np.array(X_train_augmented)\n\n# Fit the model with the augmented training data\nbatch_size = 128\nepochs = 200\nmodel.fit(X_train_augmented, y_train_categorical, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val_categorical))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.095058Z","iopub.status.idle":"2023-06-19T05:29:38.096010Z","shell.execute_reply.started":"2023-06-19T05:29:38.095660Z","shell.execute_reply":"2023-06-19T05:29:38.095681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the testing data\nscore = model.evaluate(X_test, y_test_categorical, verbose=0)\nprint(\"Testing loss:\", score[0])\nprint(\"Testing accuracy:\", score[1])\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.097318Z","iopub.status.idle":"2023-06-19T05:29:38.097918Z","shell.execute_reply.started":"2023-06-19T05:29:38.097696Z","shell.execute_reply":"2023-06-19T05:29:38.097715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Define the time shift range in number of samples\ntime_shift_range = 10\n\n# Generate augmented data by applying time shift\ndef apply_time_shift(data, shift_range):\n    augmented_data = []\n    for sample in data:\n        shift_amount = np.random.randint(-shift_range, shift_range)\n        augmented_sample = np.roll(sample, shift_amount, axis=2)\n        augmented_data.append(augmented_sample)\n    return np.array(augmented_data)\n\n# Reshape the training data to (num_samples, num_channels, num_values)\nX_train_reshaped = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2] * X_train.shape[3]))\n\n# Apply time shift augmentation to the training data\naugmented_X_train_reshaped = apply_time_shift(X_train_reshaped, time_shift_range)\n\n# Reshape the augmented data back to the original shape\naugmented_X_train = np.reshape(augmented_X_train_reshaped, (augmented_X_train_reshaped.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3]))\n\n# Concatenate the original and augmented data\nX_train_augmented = np.concatenate((X_train, augmented_X_train), axis=0)\ny_train_augmented = np.concatenate((y_train, y_train), axis=0)\n\n# Train the model with augmented data\nmodel.fit(X_train_augmented, y_train_augmented, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.099148Z","iopub.status.idle":"2023-06-19T05:29:38.099515Z","shell.execute_reply.started":"2023-06-19T05:29:38.099335Z","shell.execute_reply":"2023-06-19T05:29:38.099352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Bidirectional, LSTM, Dropout, GRU, Dense, Activation\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(dataset, labels, test_size=0.1, random_state=42)\nprint(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_val shape:\", X_val.shape)\nprint(\"y_val shape:\", y_val.shape)\n\n# Reshape the data to match the expected input shape of the model\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], -1))\nX_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], -1))\n\n# Define the input shape based on the reshaped data\ninput_shape = (X_train.shape[1], X_train.shape[2])\n\n# Define the number of classes\nnum_classes = 2  # Assuming binary classification\n\n# Build the model\nmodel = Sequential()\n\nmodel.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=input_shape))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units=256, return_sequences=True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units=64, return_sequences=True))\nmodel.add(Dropout(0.2))\n\nmodel.add(Bidirectional(GRU(units=64, return_sequences=True)))\nmodel.add(Dropout(0.2))\n\nmodel.add(GRU(units=32))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units=16))\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(units=num_classes))\nmodel.add(Activation('softmax'))\n\n# Compile the model\noptimizer = Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss=keras.losses.categorical_crossentropy, metrics=[\"accuracy\"])\nmodel.summary()\n\n# Train the model\nbatch_size = 32\nepochs = 10\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n\n# Evaluate the model on the validation data\nscore = model.evaluate(X_val, y_val, verbose=0)\nprint(\"Validation loss:\", score[0])\nprint(\"Validation accuracy:\", score[1])\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.100857Z","iopub.status.idle":"2023-06-19T05:29:38.101236Z","shell.execute_reply.started":"2023-06-19T05:29:38.101052Z","shell.execute_reply":"2023-06-19T05:29:38.101069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.regularizers import l2\nimport numpy as np\n\n# Reshape the input data\nX_train = np.reshape(X_train, (X_train.shape[0]*X_train.shape[1], X_train.shape[2], X_train.shape[3]))\nX_val = np.reshape(X_val, (X_val.shape[0]*X_val.shape[1], X_val.shape[2], X_val.shape[3]))\n\n# Build the model\ninput_shape = (X_train.shape[1], X_train.shape[2])\n\nmodel = tf.keras.Sequential()\nmodel.add(layers.LSTM(64, input_shape=input_shape, return_sequences=True))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.LSTM(64))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.102606Z","iopub.status.idle":"2023-06-19T05:29:38.103039Z","shell.execute_reply.started":"2023-06-19T05:29:38.102821Z","shell.execute_reply":"2023-06-19T05:29:38.102838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Control30_scaled.shape)\nprint(Concussed52_scaled.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.104590Z","iopub.status.idle":"2023-06-19T05:29:38.105004Z","shell.execute_reply.started":"2023-06-19T05:29:38.104783Z","shell.execute_reply":"2023-06-19T05:29:38.104800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Print the shapes of the train and test sets\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.106045Z","iopub.status.idle":"2023-06-19T05:29:38.106434Z","shell.execute_reply.started":"2023-06-19T05:29:38.106249Z","shell.execute_reply":"2023-06-19T05:29:38.106266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Reshape the input data\nX_train = X_train.reshape(-1, 60, 1000)\nX_test = X_test.reshape(-1, 60, 1000)\n\n\n# Define the ConcNet model architecture\nmodel = keras.Sequential()\n\n# Add a bidirectional LSTM layer with 64 units\nmodel.add(layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(60, 1000)))\nmodel.add(layers.Dropout(0.5))\n\n# Add a GRU layer with 32 units\nmodel.add(layers.GRU(units=32))\nmodel.add(layers.Dropout(0.5))\n\n# Add a dense layer with 16 units and ReLU activation\nmodel.add(layers.Dense(units=16, activation='relu'))\nmodel.add(layers.Dropout(0.5))\n\n# Add the output layer with 1 unit and sigmoid activation for binary classification\nmodel.add(layers.Dense(units=1, activation='sigmoid'))\n\n# Define the optimizer with a specific learning rate\noptimizer = Adam(learning_rate=0.0001)\n\n# Compile the model\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\n# Define early stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Train the model with early stopping and batch size of 256\nmodel.fit(X_train, y_train, batch_size=128, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping])\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.108163Z","iopub.status.idle":"2023-06-19T05:29:38.108894Z","shell.execute_reply.started":"2023-06-19T05:29:38.108677Z","shell.execute_reply":"2023-06-19T05:29:38.108697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train, batch_size=32, epochs=200, validation_data=(X_test, y_test))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.110110Z","iopub.status.idle":"2023-06-19T05:29:38.110486Z","shell.execute_reply.started":"2023-06-19T05:29:38.110296Z","shell.execute_reply":"2023-06-19T05:29:38.110312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = model.evaluate(X_test, y_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.111678Z","iopub.status.idle":"2023-06-19T05:29:38.112070Z","shell.execute_reply.started":"2023-06-19T05:29:38.111884Z","shell.execute_reply":"2023-06-19T05:29:38.111901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndataset = np.concatenate((all_control_power, all_concussed_power), axis=0)\nlabels = np.concatenate((np.zeros(len(all_control_power)), np.ones(len(all_concussed_power))))","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.113241Z","iopub.status.idle":"2023-06-19T05:29:38.114057Z","shell.execute_reply.started":"2023-06-19T05:29:38.113771Z","shell.execute_reply":"2023-06-19T05:29:38.113794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\n\nX = np.concatenate((all_control_power, all_concussed_power), axis=0)\n\ny = np.concatenate((np.zeros(all_control_power.shape[0] * 105), np.ones(all_concussed_power.shape[0] * 105)))\n\nX = X.reshape(-1, X.shape[2], X.shape[3])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Build the model\nmodel = Sequential()\nmodel.add(LSTM(units=64, input_shape=(X.shape[1], X.shape[2])))\nmodel.add(GRU(units=64))\nmodel.add(Dropout(0.5))  # Add dropout layer with 20% dropout rate\nmodel.add(Dense(units=1, activation='sigmoid'))\n\n# Set learning rate for optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test))\n\n# Print training and validation accuracy\ntrain_accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\n\n_, accuracy = model.evaluate(X_test, y_test)\nprint(\"Accuracy:\", accuracy)\n\ny_pred = model.predict(X_test)\ny_pred = np.round(y_pred).flatten()\n\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(cm)\n\ntarget_names = ['Control', 'Concussed']\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=target_names))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.115393Z","iopub.status.idle":"2023-06-19T05:29:38.115767Z","shell.execute_reply.started":"2023-06-19T05:29:38.115585Z","shell.execute_reply":"2023-06-19T05:29:38.115602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# num_control_samples = len(all_control_power)\n# num_concussed_samples = len(all_concussed_power)\n\n# # Randomly select a subset of samples from the larger class\n# undersampled_concussed_indices = np.random.choice(\n#     num_concussed_samples, size=num_control_samples, replace=False\n# )\n# undersampled_concussed_power = all_concussed_power[undersampled_concussed_indices]\n# undersampled_concussed_labels = labels[num_control_samples:][undersampled_concussed_indices]\n\n# # Combine the undersampled concussed data with the original control data\n# undersampled_data = np.concatenate((all_control_power, undersampled_concussed_power), axis=0)\n# undersampled_labels = np.concatenate((labels[:num_control_samples], undersampled_concussed_labels), axis=0)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.117181Z","iopub.status.idle":"2023-06-19T05:29:38.117554Z","shell.execute_reply.started":"2023-06-19T05:29:38.117367Z","shell.execute_reply":"2023-06-19T05:29:38.117384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(undersampled_concussed_power.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.118805Z","iopub.status.idle":"2023-06-19T05:29:38.119202Z","shell.execute_reply.started":"2023-06-19T05:29:38.119016Z","shell.execute_reply":"2023-06-19T05:29:38.119033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(dataset, labels, test_size=0.1, random_state=42)\nprint(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_val shape:\", X_val.shape)\nprint(\"y_val shape:\", y_val.shape)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.120553Z","iopub.status.idle":"2023-06-19T05:29:38.120935Z","shell.execute_reply.started":"2023-06-19T05:29:38.120735Z","shell.execute_reply":"2023-06-19T05:29:38.120752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\ncallback_list = EarlyStopping(\n        monitor='val_loss',\n        patience=20,\n        restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.122247Z","iopub.status.idle":"2023-06-19T05:29:38.122626Z","shell.execute_reply.started":"2023-06-19T05:29:38.122433Z","shell.execute_reply":"2023-06-19T05:29:38.122449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.regularizers import l2\nimport numpy as np\n\n# Reshape the input data\nX_train = np.reshape(X_train, (X_train.shape[0]*X_train.shape[1], X_train.shape[2], X_train.shape[3]))\nX_val = np.reshape(X_val, (X_val.shape[0]*X_val.shape[1], X_val.shape[2], X_val.shape[3]))\n\n# Build the model\ninput_shape = (X_train.shape[1], X_train.shape[2])\n\nmodel = tf.keras.Sequential()\nmodel.add(layers.LSTM(64, input_shape=input_shape, return_sequences=True))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.LSTM(64))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.123849Z","iopub.status.idle":"2023-06-19T05:29:38.124247Z","shell.execute_reply.started":"2023-06-19T05:29:38.124061Z","shell.execute_reply":"2023-06-19T05:29:38.124079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    loss='binary_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n    metrics=['accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.125830Z","iopub.status.idle":"2023-06-19T05:29:38.126578Z","shell.execute_reply.started":"2023-06-19T05:29:38.126379Z","shell.execute_reply":"2023-06-19T05:29:38.126398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# from sklearn.model_selection import KFold\n# from sklearn.metrics import classification_report, confusion_matrix\n# import numpy as np\n# import tensorflow as tf\n# from tensorflow.keras import layers\n\n# X = dataset\n# y = labels\n# Epoch = 200\n# batchSize = 16\n\n# k_folds = 2\n# kf = KFold(n_splits=k_folds, shuffle=True)\n\n# train_accuracies = []\n# val_accuracies = []\n# cm_total = np.zeros((2, 2))\n# y_preds = []\n\n# # Define EarlyStopping callback\n# early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# for train_index, val_index in kf.split(X):\n#     X_train, X_val = X[train_index], X[val_index]\n#     y_train, y_val = y[train_index], y[val_index]\n\n#     # Build and compile the model\n#     model = tf.keras.Sequential()\n#     model.add(layers.LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]*X_train.shape[3]), return_sequences=True))\n#     model.add(layers.Dropout(0.7))\n#     model.add(layers.LSTM(64))\n#     model.add(layers.Dropout(0.7))\n#     model.add(layers.Dense(1, activation='sigmoid'))\n#     model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n\n#     history = model.fit(\n#         X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2]*X_train.shape[3]),\n#         y_train,\n#         epochs=Epoch,\n#         batch_size=batchSize,\n#         validation_data=(X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2]*X_val.shape[3]), y_val),\n#         callbacks=[early_stop],  # Add the EarlyStopping callback\n#         verbose=1\n#     )\n\n#     train_loss, train_accuracy = model.evaluate(X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2]*X_train.shape[3]), y_train)\n#     val_loss, val_accuracy = model.evaluate(X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2]*X_val.shape[3]), y_val)\n\n#     train_accuracies.append(train_accuracy)\n#     val_accuracies.append(val_accuracy)\n\n#     y_pred = model.predict(X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2]*X_val.shape[3]))\n#     y_pred = np.round(y_pred).flatten()\n#     y_preds.append(y_pred)\n\n#     cm = confusion_matrix(y_val, y_pred)\n#     cm_total += cm\n\n# avg_train_accuracy = np.mean(train_accuracies)\n# avg_val_accuracy = np.mean(val_accuracies)\n# cm_avg = cm_total / k_folds\n\n# y_preds = np.concatenate(y_preds)\n\n# print('Confusion Matrix')\n# print(cm_total)\n# print('\\n')\n\n# # Define target names\n# target_names = ['Control', 'Concussed']\n\n# # Calculate classification report\n# print('Classification Report')\n# print(classification_report(y, y_preds, target_names=target_names))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.128051Z","iopub.status.idle":"2023-06-19T05:29:38.128418Z","shell.execute_reply.started":"2023-06-19T05:29:38.128236Z","shell.execute_reply":"2023-06-19T05:29:38.128252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import tensorflow as tf\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Conv2D, LSTM, Flatten, Dense\n\n# # Assuming X and y are your EEG dataset and corresponding labels\n# X = np.random.randn(105, 60, 1004)  # Replace with your actual data\n# y = np.random.randint(0, 2, size=(105,))  # Replace with your actual labels\n\n# # Reshape the data for 2D CNN\n# X_2d_cnn = X.reshape(105, 60, 1004, 1)\n\n# # Define 2D CNN model\n# model_2d_cnn = Sequential()\n# model_2d_cnn.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(60, 1004, 1)))\n# model_2d_cnn.add(Flatten())\n# model_2d_cnn.add(Dense(128, activation='relu'))\n# model_2d_cnn.add(Dense(1, activation='sigmoid'))\n# model_2d_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# model_2d_cnn.summary()\n\n# # Fit the 2D CNN model\n# model_2d_cnn.fit(X_2d_cnn, y, epochs=10, batch_size=32)\n\n# # Reshape the data for LSTM\n# X_lstm = X.reshape(105, 1004, 60)\n\n# # Define LSTM model\n# model_lstm = Sequential()\n# model_lstm.add(LSTM(units=64, input_shape=(1004, 60)))\n# model_lstm.add(Dense(128, activation='relu'))\n# model_lstm.add(Dense(1, activation='sigmoid'))\n# model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# model_lstm.summary()\n\n# # Fit the LSTM model\n# model_lstm.fit(X_lstm, y, epochs=10, batch_size=32)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.129538Z","iopub.status.idle":"2023-06-19T05:29:38.129952Z","shell.execute_reply.started":"2023-06-19T05:29:38.129723Z","shell.execute_reply":"2023-06-19T05:29:38.129740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nX = dataset\ny = labels\nEpoch = 100\nbatchSize = 16\n\nk_folds = 2\nkf = KFold(n_splits=k_folds, shuffle=True)\n\ntrain_accuracies = []\nval_accuracies = []\ncm_total = np.zeros((2, 2))\ny_preds = []\n\n# Define EarlyStopping callback\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\nfor train_index, val_index in kf.split(X):\n    X_train, X_val = X[train_index], X[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n\n    # Build the CNN model\n    cnn_model = models.Sequential()\n    cnn_model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(105, 60, 1004)))\n    cnn_model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n    cnn_model.add(layers.MaxPooling2D(pool_size=(2, 2), padding='same'))\n    cnn_model.add(layers.BatchNormalization())\n    cnn_model.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n    cnn_model.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n    cnn_model.add(layers.MaxPooling2D(pool_size=(2, 2), padding='same'))\n    cnn_model.add(layers.Dropout(0.2))\n    cnn_model.add(layers.Flatten())\n\n    # Build the LSTM model\n    lstm_model = models.Sequential()\n    lstm_model.add(layers.LSTM(units=128, input_shape=(13, 4)))\n    lstm_model.add(layers.Dense(50, activation='relu'))\n\n    # Combine the CNN and LSTM models\n    combined = layers.concatenate([cnn_model.output, lstm_model.output])\n    combined = layers.Dense(units=50, activation='relu')(combined)\n    output_layer = layers.Dense(7, activation='softmax')(combined)\n\n    # Create the final model\n    model = models.Model(inputs=[cnn_model.input, lstm_model.input], outputs=output_layer)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    # Train the model\n    history = model.fit(\n        [X_train_cnn, X_train_lstm],\n        y_train,\n        epochs=Epoch,\n        batch_size=batchSize,\n        validation_data=([X_val_cnn, X_val_lstm], y_val),\n        callbacks=[early_stop],\n        verbose=1\n    )\n\n    train_loss, train_accuracy = model.evaluate(X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2]*X_train.shape[3]), y_train)\n    val_loss, val_accuracy = model.evaluate(X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2]*X_val.shape[3]), y_val)\n\n\n    train_accuracies.append(train_accuracy)\n    val_accuracies.append(val_accuracy)\n\n    y_pred = model.predict([X_val_cnn, X_val_lstm])\n    y_pred = np.argmax(y_pred, axis=1)\n    y_preds.append(y_pred)\n\n    cm = confusion_matrix(np.argmax(y_val, axis=1), y_pred)\n    cm_total += cm\n\navg_train_accuracy = np.mean(train_accuracies)\navg_val_accuracy = np.mean(val_accuracies)\ncm_avg = cm_total / k_folds\n\ny_preds = np.concatenate(y_preds)\n\nprint('Confusion Matrix')\nprint(cm_total)\nprint('\\n')\n\n# Define target names\ntarget_names = ['Normal', 'Concussed']\n\n# Calculate classification report\nprint('Classification Report')\nprint(classification_report(np.argmax(y, axis=1), y_preds, target_names=target_names))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.131193Z","iopub.status.idle":"2023-06-19T05:29:38.131565Z","shell.execute_reply.started":"2023-06-19T05:29:38.131380Z","shell.execute_reply":"2023-06-19T05:29:38.131397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the average accuracies\navg_train_accuracy = np.mean(train_accuracies)\navg_val_accuracy = np.mean(val_accuracies)\n\nprint('Average Training Accuracy:', avg_train_accuracy)\nprint('Average Validation Accuracy:', avg_val_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.132835Z","iopub.status.idle":"2023-06-19T05:29:38.133231Z","shell.execute_reply.started":"2023-06-19T05:29:38.133046Z","shell.execute_reply":"2023-06-19T05:29:38.133064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Confusion Matrix')\nprint(cm_total)\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.134580Z","iopub.status.idle":"2023-06-19T05:29:38.135003Z","shell.execute_reply.started":"2023-06-19T05:29:38.134769Z","shell.execute_reply":"2023-06-19T05:29:38.134786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot training and validation accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\n# Plot training and validation loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Validation'], loc='upper right')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.137198Z","iopub.status.idle":"2023-06-19T05:29:38.137588Z","shell.execute_reply.started":"2023-06-19T05:29:38.137401Z","shell.execute_reply":"2023-06-19T05:29:38.137419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\ntarget_names = ['Normal', 'Concussed']\nprint('Confusion Matrix')\ncm = confusion_matrix(true_labels, predicted_labels)\nprint(cm)\nprint('\\n')\n\nprint('Classification Report')\nprint(classification_report(true_labels, predicted_labels, target_names=target_names))","metadata":{"execution":{"iopub.status.busy":"2023-06-19T05:29:38.138967Z","iopub.status.idle":"2023-06-19T05:29:38.139350Z","shell.execute_reply.started":"2023-06-19T05:29:38.139164Z","shell.execute_reply":"2023-06-19T05:29:38.139182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}